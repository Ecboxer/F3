{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO Get newly generated data in shape\n",
    "- TODO Run AlignScore\n",
    "- TODO Filter on AlignScore\n",
    "- TODO Run GPT-3.5, GPT-4o, Llama2 detection on filtered results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>human_written_content</th>\n",
       "      <th>aigenerated_content</th>\n",
       "      <th>model</th>\n",
       "      <th>num_completion_token</th>\n",
       "      <th>num_original_token</th>\n",
       "      <th>num_prompt_token</th>\n",
       "      <th>num_iagenerated_token</th>\n",
       "      <th>original_label</th>\n",
       "      <th>source_type</th>\n",
       "      <th>ai_generated_label</th>\n",
       "      <th>article_type</th>\n",
       "      <th>pre_post_GPT</th>\n",
       "      <th>dataset_source</th>\n",
       "      <th>Prompt_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9336c482-793a-4167-ae57-c67d17355381</td>\n",
       "      <td>The shooting of 18-year-old Michael Brown is a...</td>\n",
       "      <td>The fatal shooting of 18-year-old Michael Brow...</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>349</td>\n",
       "      <td>958</td>\n",
       "      <td>1133</td>\n",
       "      <td>1482</td>\n",
       "      <td>real</td>\n",
       "      <td>AI Machine</td>\n",
       "      <td>fake</td>\n",
       "      <td>news article</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>FakeNewsNet_Politifacts</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346510d8-e2ad-4d97-801a-c339ac8e0c85</td>\n",
       "      <td>Expanding Opportunity — #KempForum16Let’s get ...</td>\n",
       "      <td>Republican senator Tim Scott and House Speaker...</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>442</td>\n",
       "      <td>1116</td>\n",
       "      <td>1210</td>\n",
       "      <td>1652</td>\n",
       "      <td>real</td>\n",
       "      <td>AI Machine</td>\n",
       "      <td>fake</td>\n",
       "      <td>news article</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>FakeNewsNet_Politifacts</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33018c69-3cd5-4971-adc0-d1656645b736</td>\n",
       "      <td>Amid the numerous reports of events in Ukraine...</td>\n",
       "      <td>Recent developments in Crimea have captured th...</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>377</td>\n",
       "      <td>994</td>\n",
       "      <td>1243</td>\n",
       "      <td>1620</td>\n",
       "      <td>real</td>\n",
       "      <td>AI Machine</td>\n",
       "      <td>fake</td>\n",
       "      <td>news article</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>FakeNewsNet_Politifacts</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid  \\\n",
       "0  9336c482-793a-4167-ae57-c67d17355381   \n",
       "1  346510d8-e2ad-4d97-801a-c339ac8e0c85   \n",
       "2  33018c69-3cd5-4971-adc0-d1656645b736   \n",
       "\n",
       "                               human_written_content  \\\n",
       "0  The shooting of 18-year-old Michael Brown is a...   \n",
       "1  Expanding Opportunity — #KempForum16Let’s get ...   \n",
       "2  Amid the numerous reports of events in Ukraine...   \n",
       "\n",
       "                                 aigenerated_content               model  \\\n",
       "0  The fatal shooting of 18-year-old Michael Brow...  gpt-3.5-turbo-0125   \n",
       "1  Republican senator Tim Scott and House Speaker...  gpt-3.5-turbo-0125   \n",
       "2  Recent developments in Crimea have captured th...  gpt-3.5-turbo-0125   \n",
       "\n",
       "   num_completion_token  num_original_token  num_prompt_token  \\\n",
       "0                   349                 958              1133   \n",
       "1                   442                1116              1210   \n",
       "2                   377                 994              1243   \n",
       "\n",
       "   num_iagenerated_token original_label source_type ai_generated_label  \\\n",
       "0                   1482           real  AI Machine               fake   \n",
       "1                   1652           real  AI Machine               fake   \n",
       "2                   1620           real  AI Machine               fake   \n",
       "\n",
       "   article_type pre_post_GPT           dataset_source Prompt_type  \n",
       "0  news article      pre-GPT  FakeNewsNet_Politifacts    Critical  \n",
       "1  news article      pre-GPT  FakeNewsNet_Politifacts    Critical  \n",
       "2  news article      pre-GPT  FakeNewsNet_Politifacts    Critical  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\n",
    "    'gpt3_5',\n",
    "    'gpt4o',\n",
    "    'llama2',\n",
    "]\n",
    "prompt_types = [\n",
    "    'Critical',\n",
    "    'Major',\n",
    "    'Minor',\n",
    "]\n",
    "dfs = []\n",
    "for model in models:\n",
    "    for prompt_type in prompt_types:\n",
    "        try:\n",
    "            df_real = pd.read_csv(\n",
    "                f'X_GenPost_{model}_Real_Post_Completed_Data/{prompt_type}_results.csv'\n",
    "            )\n",
    "            df_fake = pd.read_csv(\n",
    "                f'X_GenPost_{model}_Fake_Post_Completed_Data/{prompt_type}_results.csv'\n",
    "            )\n",
    "\n",
    "            df_real.loc[:,'Prompt_type'] = prompt_type\n",
    "            df_fake.loc[:,'Prompt_type'] = prompt_type\n",
    "            \n",
    "            dfs.append(df_real)\n",
    "            dfs.append(df_fake)\n",
    "        except FileNotFoundError:\n",
    "            print(f'model: {model}; prompt_type {prompt_type} not found')\n",
    "df = pd.concat(dfs)\n",
    "print(df.shape)\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 36000 entries, 0 to 999\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   uuid            36000 non-null  object\n",
      " 1   label           36000 non-null  object\n",
      " 2   article_type    36000 non-null  object\n",
      " 3   source_type     36000 non-null  object\n",
      " 4   pre_post_GPT    36000 non-null  object\n",
      " 5   dataset_source  36000 non-null  object\n",
      " 6   Prompt_type     36000 non-null  object\n",
      " 7   content         35959 non-null  object\n",
      " 8   text_length     36000 non-null  int64 \n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Create version with just the pre-scoring columns\n",
    "cols_shared = [\n",
    "    'uuid', 'label', 'article_type', 'source_type', 'pre_post_GPT',\n",
    "    'dataset_source', 'Prompt_type',\n",
    "]\n",
    "df_gen_wide = df.rename(columns={\n",
    "    'ai_generated_label': 'label',\n",
    "})[cols_shared + [\n",
    "    'human_written_content', 'aigenerated_content',  # -> content\n",
    "    'num_original_token', 'num_completion_token',  # -> text_length\n",
    "]]\n",
    "\n",
    "# Human text\n",
    "df_gen_human = df_gen_wide.rename(columns={\n",
    "    'human_written_content': 'content',\n",
    "    'num_original_token': 'text_length',\n",
    "})[cols_shared + ['content', 'text_length']]\n",
    "df_gen_human.loc[:,'source_type'] = 'human'\n",
    "\n",
    "# LLM/AI text\n",
    "df_gen_llm = df_gen_wide.rename(columns={\n",
    "    'aigenerated_content': 'content',\n",
    "    'num_completion_token': 'text_length',\n",
    "})[cols_shared + ['content', 'text_length']]\n",
    "df_gen_llm.loc[:,'source_type'] = 'LLM'\n",
    "\n",
    "df_gen = pd.concat([df_gen_human, df_gen_llm])\n",
    "df_gen.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3600 entries, 469 to 318\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   uuid            3600 non-null   object\n",
      " 1   label           3600 non-null   object\n",
      " 2   article_type    3600 non-null   object\n",
      " 3   source_type     3600 non-null   object\n",
      " 4   pre_post_GPT    3600 non-null   object\n",
      " 5   dataset_source  3600 non-null   object\n",
      " 6   Prompt_type     3600 non-null   object\n",
      " 7   content         3598 non-null   object\n",
      " 8   text_length     3600 non-null   int64 \n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 281.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create subsetted version for replication\n",
    "\n",
    "# Create version with just the pre-scoring columns\n",
    "cols_shared = [\n",
    "    'uuid', 'label', 'article_type', 'source_type', 'pre_post_GPT',\n",
    "    'dataset_source', 'Prompt_type',\n",
    "]\n",
    "df_gen_wide = df.rename(columns={\n",
    "    'ai_generated_label': 'label',\n",
    "})[cols_shared + [\n",
    "    'human_written_content', 'aigenerated_content',  # -> content\n",
    "    'num_original_token', 'num_completion_token',  # -> text_length\n",
    "]]\n",
    "\n",
    "df_gen_wide_ss = df_gen_wide.sample(frac=.1, random_state=0)\n",
    "\n",
    "# Human text\n",
    "df_gen_human_ss = df_gen_wide_ss.rename(columns={\n",
    "    'human_written_content': 'content',\n",
    "    'num_original_token': 'text_length',\n",
    "})[cols_shared + ['content', 'text_length']]\n",
    "df_gen_human_ss.loc[:,'source_type'] = 'human'\n",
    "\n",
    "# LLM/AI text\n",
    "df_gen_llm_ss = df_gen_wide_ss.rename(columns={\n",
    "    'aigenerated_content': 'content',\n",
    "    'num_completion_token': 'text_length',\n",
    "})[cols_shared + ['content', 'text_length']]\n",
    "df_gen_llm_ss.loc[:,'source_type'] = 'LLM'\n",
    "\n",
    "df_gen_ss = pd.concat([df_gen_human_ss, df_gen_llm_ss])\n",
    "df_gen_ss.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>label</th>\n",
       "      <th>article_type</th>\n",
       "      <th>source_type</th>\n",
       "      <th>pre_post_GPT</th>\n",
       "      <th>dataset_source</th>\n",
       "      <th>Prompt_type</th>\n",
       "      <th>content</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9336c482-793a-4167-ae57-c67d17355381</td>\n",
       "      <td>fake</td>\n",
       "      <td>news article</td>\n",
       "      <td>human</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>FakeNewsNet_Politifacts</td>\n",
       "      <td>Critical</td>\n",
       "      <td>The shooting of 18-year-old Michael Brown is a...</td>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346510d8-e2ad-4d97-801a-c339ac8e0c85</td>\n",
       "      <td>fake</td>\n",
       "      <td>news article</td>\n",
       "      <td>human</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>FakeNewsNet_Politifacts</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Expanding Opportunity — #KempForum16Let’s get ...</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33018c69-3cd5-4971-adc0-d1656645b736</td>\n",
       "      <td>fake</td>\n",
       "      <td>news article</td>\n",
       "      <td>human</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>FakeNewsNet_Politifacts</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Amid the numerous reports of events in Ukraine...</td>\n",
       "      <td>994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid label  article_type source_type  \\\n",
       "0  9336c482-793a-4167-ae57-c67d17355381  fake  news article       human   \n",
       "1  346510d8-e2ad-4d97-801a-c339ac8e0c85  fake  news article       human   \n",
       "2  33018c69-3cd5-4971-adc0-d1656645b736  fake  news article       human   \n",
       "\n",
       "  pre_post_GPT           dataset_source Prompt_type  \\\n",
       "0      pre-GPT  FakeNewsNet_Politifacts    Critical   \n",
       "1      pre-GPT  FakeNewsNet_Politifacts    Critical   \n",
       "2      pre-GPT  FakeNewsNet_Politifacts    Critical   \n",
       "\n",
       "                                             content  text_length  \n",
       "0  The shooting of 18-year-old Michael Brown is a...          958  \n",
       "1  Expanding Opportunity — #KempForum16Let’s get ...         1116  \n",
       "2  Amid the numerous reports of events in Ukraine...          994  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gen[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_type\n",
       "human    18000\n",
       "LLM      18000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gen['source_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_type\n",
       "human    1800\n",
       "LLM      1800\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gen_ss['source_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAfyFVZjlEw9"
   },
   "source": [
    "# Classification Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mh8FZZraheyX"
   },
   "source": [
    "## Cloze Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DjvrQjdUCYAW"
   },
   "outputs": [],
   "source": [
    "\"\"\"Generate a prompt using template.\n",
    "\n",
    "  Args:\n",
    "    F3 prompt template (θ) has three parameters: (1) content (C) embeds data to be analyzed. The content parameter in the the function [ai_text_generator] (2) Impersonator (R)\n",
    "    establishes context, guides LLMs’ generation and detection, and overrides alignment-tuning. (3) Instructor (I)\n",
    "    provides directives to guide LLM.\n",
    "\n",
    "  Returns:\n",
    "    A generated prompt.\n",
    "\"\"\"\n",
    "def get_prompt(prompt_type):\n",
    "\n",
    "  impersonator = \"\"\n",
    "  instructor = \"\"\n",
    "  cloze_prompt = \"Is this real or fake news? {_____}\"\n",
    "\n",
    "  if prompt_type == \"VaN\":# Technique\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news.\"\n",
    "    instructor = \"Analyze the given text and determine if it is real or fake news.\"\n",
    "\n",
    "  elif prompt_type == \"A_CoT\":# Technique  # Z_CoT\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news.\"\n",
    "    instructor = \"Analyze the given text, think step-by-step, and determine if it is real or fake news.\"\n",
    "\n",
    "  elif prompt_type == \"X_CoT\":# Technique\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news.\"\n",
    "    instructor = \"Analyze the given text, explain your reasoning step-by-step, and determine if it is real or fake news.\"\n",
    "\n",
    "  elif prompt_type == \"A_Con\":# Technique\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news with confidence estimates.\"\n",
    "    instructor = \"Analyze the given text, provide a confidence score between 0-100%, and determine if it is real or fake news.\"\n",
    "\n",
    "  elif prompt_type == \"MSReN\":# Technique\n",
    "    impersonator = \"You are an AI fact checker trained to detect fake news.\"\n",
    "    instructor = \"Analyze the text in detail as a fact checker would. Check for misleading info, false claims, biased language. If real, respond 'True', if fake, respond 'False'.\"\n",
    "  \n",
    "  # From paper\n",
    "  elif prompt_type == \"MSReN_paper\":# Technique\n",
    "    impersonator = \"You are an AI fact checker trained to detect fake news.\"\n",
    "    instructor = \"Analyze the text in detail as a fact checker would solve it by splitting your reasoning into steps. Check for misleading info, false claims, biased language. If real, respond ’True’, if fake, respond ’False’.\"\n",
    "  \n",
    "  elif prompt_type == \"MSReN_CoT\":# Technique\n",
    "    impersonator = \"You are an AI fact checker trained to detect fake news.\"\n",
    "    instructor = \"Analyze the text in detail as a fact checker would. Explain your reasoning step-by-step. If real, respond 'True', if fake, respond 'False'.\"\n",
    "\n",
    "  elif prompt_type == \"DeF_Gen\":# Technique\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news.\"\n",
    "    instructor = \"Determine if the text contains factual information supported by evidence (real) or misleading/inaccurate information (fake). Respond with 'True' or 'False'.\"\n",
    "\n",
    "  elif prompt_type == \"DeF_SpeC\":# Technique\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news.\"\n",
    "    instructor = \"Analyze the tone, language, sources to determine if the text is real (supported by facts) or fake (misleading info). Respond with 'True' or 'False'.\"\n",
    "\n",
    "  elif prompt_type == \"Analyze_Cld2\":# Technique\n",
    "    impersonator = \"You are an AI assistant specially trained to detect fake news with high accuracy.\"\n",
    "    instructor = \"\"\"Analyze the given news article in depth. Check for the following indicators that it may be fake news:\n",
    "    - Inaccurate or misleading information\n",
    "    - Lack of evidence for claims\n",
    "    - Emotionally charged language with intent to provoke outrage or shock\n",
    "    - Biased portrayal of events or people\n",
    "    - Unverified sources or 'experts'\n",
    "    - Logical fallacies or conspiracy theories without proof\n",
    "\n",
    "    If the article exhibits multiple indicators of fake news, respond 'False'. If the article is factual and supported by credible evidence, respond 'True'. Your judgement should be highly accurate.\n",
    "    \"\"\"\n",
    "\n",
    "  elif prompt_type == \"Analyze_AI_GPT\":# Technique\n",
    "    impersonator = \"You are an AI trained with extensive knowledge up to 2022 on various news articles, both real and fake.\"\n",
    "    instructor = \"\"\"Analyze the given text for potential indicators of fake news, such as:\n",
    "    - Sensationalist or emotionally charged language.\n",
    "    - Absence of specific details or dates.\n",
    "    - Over-generalizations or sweeping statements.\n",
    "    - Statements that are too good to be true or overly dramatic.\n",
    "    - Lack of logical flow in arguments or jumping to conclusions without evidence.\n",
    "\n",
    "    It's essential to understand that without real-time verification capabilities, your judgment will be based on patterns and knowledge up to your last training. Using these textual cues and your training, determine the credibility of the given text. If it seems factual and consistent with your training, respond 'True'. If it exhibits patterns typical of fake news, respond 'False'.\"\"\"\n",
    "\n",
    "  else:\n",
    "    raise ValueError('Unexpected prompt_type:', prompt_type)\n",
    "    \n",
    "  prompt = f\"{impersonator} {instructor} {cloze_prompt}\"\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms3R06B6RKU6"
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve7y5Y1TTwGG"
   },
   "source": [
    "## DeepInfra\n",
    "\n",
    "\n",
    "We using [DeepInfra](https://deepinfra.com/) API for inference with the following LLMs.\n",
    "\n",
    "\n",
    "1.   LLaMa-2: meta-llama/Llama-2-70b-chat-hf\n",
    "2.   Dolly-2: databricks/dolly-v2-12b\n",
    "3.   Falcon: tiiuae/falcon-40b-q51\n",
    "4.   LLaMa-2-GPT: jondurbin/airoboros-l2-70b-gpt4-1.4.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMQmSgXG-FLF"
   },
   "source": [
    "### TODO Llama-2 70b, Dolly-2, LLaMA-GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "# import openai\n",
    "# from openai import AsyncOpenAI\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "# import concurrent.futures\n",
    "import nest_asyncio  # To run asyncio in Jupyter notebooks\n",
    "import asyncio\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "HF_API_TOKEN = os.getenv('HF_KEY')\n",
    "client = InferenceClient(api_key=HF_API_TOKEN)\n",
    "\n",
    "# Set the prompt pattern\n",
    "prompt_types = [\n",
    "    \"VaN\", \"A_CoT\", \"X_CoT\", \"A_Con\",\n",
    "    \"MSReN\", \"MSReN_paper\", \"MSReN_CoT\",\n",
    "    \"DeF_Gen\", \"DeF_SpeC\",\n",
    "    \"Analyze_Cld2\", \"Analyze_AI_GPT\"\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Replace the model name below with:\n",
    "1.   LLaMa-2: meta-llama/Llama-2-70b-chat-hf             # TODO\n",
    "2.   Dolly-2: databricks/dolly-v2-12b                    # TODO\n",
    "3.   LLaMa-2-GPT: jondurbin/airoboros-l2-70b-gpt4-1.4.1  # TODO\n",
    "\"\"\"\n",
    "model_name = 'llama2'\n",
    "model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "api_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 403 - {\"error\":\"The model jondurbin/airoboros-l2-7b-gpt4-1.4.1 is too large to be loaded automatically (13GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "HF_API_TOKEN = os.getenv(\"HF_KEY\")\n",
    "headers = {\"Authorization\": f\"Bearer {HF_API_TOKEN}\"}\n",
    "api_url = \"https://api-inference.huggingface.co/models/jondurbin/airoboros-l2-7b-gpt4-1.4.1\"\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 50,\n",
    "    }\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "response = requests.post(api_url, headers=headers, json=payload)\n",
    "end_time = time.time()\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"Response time: {end_time - start_time:.2f} seconds\")\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5534, 9)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gen_ss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutputMessage(role='assistant', content='The capital of France is Paris.', tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qq5Io42G5Uu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  18%|█▊        | 2/11 [00:00<00:00, 12.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n",
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n",
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  36%|███▋      | 4/11 [00:00<00:00, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n",
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n",
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  55%|█████▍    | 6/11 [00:00<00:00, 12.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n",
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  73%|███████▎  | 8/11 [00:04<00:02,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  82%|████████▏ | 9/11 [00:06<00:02,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  91%|█████████ | 10/11 [00:07<00:01,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0: 100%|██████████| 11/11 [00:09<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "# import openai\n",
    "# from openai import AsyncOpenAI\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "# import concurrent.futures\n",
    "import nest_asyncio  # To run asyncio in Jupyter notebooks\n",
    "import asyncio\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "HF_API_TOKEN = os.getenv('HF_KEY')\n",
    "client = InferenceClient(api_key=HF_API_TOKEN)\n",
    "\n",
    "# Set the prompt pattern\n",
    "prompt_types = [\n",
    "    \"VaN\", \"A_CoT\", \"X_CoT\", \"A_Con\",\n",
    "    \"MSReN\", \"MSReN_paper\", \"MSReN_CoT\",\n",
    "    \"DeF_Gen\", \"DeF_SpeC\",\n",
    "    \"Analyze_Cld2\", \"Analyze_AI_GPT\"\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Replace the model name below with:\n",
    "# Note: Llama-2-70b unavailable on HuggingFace\n",
    "1.   LLaMa-2: meta-llama/Llama-2-7b-chat-hf              # TODO\n",
    "2.   Dolly-2: databricks/dolly-v2-12b                    # TODO\n",
    "3.   LLaMa-2-GPT: jondurbin/airoboros-l2-70b-gpt4-1.4.1  # TODO\n",
    "4.   (New) Mixtral: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "\"\"\"\n",
    "model_name = 'llama2'\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "api_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "\n",
    "seeds = [0]\n",
    "for seed in seeds:\n",
    "    # Ensure backup directory exists\n",
    "    backup_folder = f'bkp_{model_name}_seed_{seed}'\n",
    "    os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "    # Load previous progress if exists\n",
    "    backup_path = os.path.join(backup_folder, \"progress.csv\")\n",
    "    last_processed_uuid = None\n",
    "    last_processed_prompt = None\n",
    "\n",
    "    try:\n",
    "        progress_df = pd.read_csv(backup_path)\n",
    "        last_processed_uuid = progress_df.iloc[-1]['uuid']\n",
    "        last_processed_prompt = progress_df.iloc[-1]['Prompt_type']\n",
    "    except FileNotFoundError:\n",
    "        progress_df = pd.DataFrame(columns=[\n",
    "            'uuid', f'{model_name}_label', 'content', 'label',\n",
    "            'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'\n",
    "        ])\n",
    "\n",
    "    # Create a list of all combinations of rows and prompt types, using progress_df if found\n",
    "    if last_processed_uuid and last_processed_prompt:\n",
    "        start_idx = df_gen_ss[df_gen_ss['uuid'] == last_processed_uuid].index[0]\n",
    "        start_prompt_idx = prompt_types.index(last_processed_prompt) + 1\n",
    "        if start_prompt_idx >= len(prompt_types):\n",
    "            start_idx += 1\n",
    "            start_prompt_idx = 0\n",
    "\n",
    "        all_data = [\n",
    "            (row, prompt)\n",
    "            for _, row in df_gen_ss[:1].iloc[start_idx:].iterrows()\n",
    "            for prompt in (prompt_types[start_prompt_idx:]\n",
    "            if row['uuid'] == last_processed_uuid else prompt_types)\n",
    "        ]\n",
    "    else:\n",
    "        all_data = [\n",
    "            (row, prompt)\n",
    "            for _, row in df_gen_ss[:1].iterrows()\n",
    "            for prompt in prompt_types\n",
    "        ]\n",
    "\n",
    "    # Function to process each row for a given prompt type\n",
    "    def process_row_for_prompt(\n",
    "        data, model_id: str, retries: int = 3, delay: float = 2., seed: int = None,\n",
    "    ):\n",
    "        row, prompt_type = data\n",
    "        content = row['content']\n",
    "        cloze_prompt = get_prompt(prompt_type)\n",
    "        \n",
    "        # Amended version without {______} in prompt\n",
    "        prompt_content = f\"Content: '{content}' prompt: '{cloze_prompt}. Please state a single probable answer if you are uncertain, lack evidence or unverified, but please answer with your single word: 'True' or 'False'.'\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_content,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                print(f\"Attempt {attempt + 1}, sending request for UUID {row['uuid']}\")\n",
    "\n",
    "                # # Text generation API request\n",
    "                # response = client.text_generation(\n",
    "                #     model=model_id,\n",
    "                #     prompt=prompt_content,\n",
    "                #     max_new_tokens=50,\n",
    "                #     seed=seed,\n",
    "                # )\n",
    "                # model_output = response.\n",
    "\n",
    "                # Chat completion API request\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=messages, \n",
    "                    max_tokens=100,  # Just expect True/False output\n",
    "                )\n",
    "                model_output = completion.choices[0].message\n",
    "\n",
    "                return (\n",
    "                    row['uuid'],\n",
    "                    model_output.content.strip(),\n",
    "                    content,\n",
    "                    row['label'],\n",
    "                    prompt_type,\n",
    "                    row['pre_post_GPT'],\n",
    "                    row['article_type'],\n",
    "                    row['dataset_source'],\n",
    "                    row['source_type']\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error for UUID {row['uuid']}: {e}\")\n",
    "                if hasattr(e, \"response\"):\n",
    "                    print(f\"Response details: {json.dumps(e.response.text, indent=2)}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    print(f\"Max retries reached for UUID {row['uuid']}\")\n",
    "                    return \"Error\"\n",
    "\n",
    "    # Process all data\n",
    "    results = []\n",
    "    for data in tqdm(all_data, total=len(all_data), desc=f'Seed {seed}'):\n",
    "        results.append(process_row_for_prompt(data, model_id))\n",
    "\n",
    "        # Adjust the save condition (you might want to modify this based on the frequency of saving)\n",
    "        if len(results) % 100 == 0 or len(results) == len(all_data):\n",
    "            print(\"Saving progress.\")\n",
    "            columns = [\n",
    "                'uuid', f'{model_name}_label', 'content', 'label',\n",
    "                'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'\n",
    "            ]\n",
    "            temp_df = pd.DataFrame(results, columns=columns)\n",
    "            progress_df = pd.concat([progress_df, temp_df], axis=0, ignore_index=True)\n",
    "            progress_df.to_csv(backup_path, mode='w', header=True, index=False)\n",
    "\n",
    "    # Save results after processing\n",
    "    columns = [\n",
    "        'uuid', f'{model_name}_label', 'content', 'label',\n",
    "        'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'\n",
    "    ]\n",
    "    temp_df = pd.DataFrame(results, columns=columns)\n",
    "    progress_df = pd.concat([progress_df, temp_df], axis=0, ignore_index=True)\n",
    "    progress_df.to_csv(backup_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCaNkpHMSHBG"
   },
   "source": [
    "### TODO Falcon-40b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKAGTphqSGnd"
   },
   "outputs": [],
   "source": [
    "# !pip install requests\n",
    "\n",
    "# Falcon is accessible via python request package below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "I3HFM0l1SYLa"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# import requests\n",
    "# from tqdm import tqdm\n",
    "# import concurrent.futures\n",
    "\n",
    "# # Set the prompt pattern\n",
    "# prompt_types = [\"VaN\", \"A_CoT\", \"X_CoT\", \"A_Con\", \"MSReN\", \"MSReN_CoT\", \"DeF_Gen\", \"DeF_SpeC\", \"Analyze_Cld2\", \"Analyze_AI_GPT\"]\n",
    "\n",
    "# # Token for authentication\n",
    "# DEEPINFRA_TOKEN = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "# # Ensure backup directory exists\n",
    "# backup_folder = \"Falcon2\"\n",
    "# os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "# # Load previous progress if exists\n",
    "# backup_path = os.path.join(backup_folder, \"progress.csv\")\n",
    "# last_processed_uuid = None\n",
    "# last_processed_prompt = None\n",
    "\n",
    "# try:\n",
    "#     progress_df = pd.read_csv(backup_path)\n",
    "#     last_processed_uuid = progress_df.iloc[-1]['uuid']\n",
    "#     last_processed_prompt = progress_df.iloc[-1]['Prompt_type']\n",
    "# except FileNotFoundError:\n",
    "#     progress_df = pd.DataFrame(columns=['uuid', 'Falcon2-label', 'content', 'label', 'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'])\n",
    "\n",
    "# # Create a list of all combinations of rows and prompt types\n",
    "# # ... [existing code for this part, which wasn't provided in your snippets] ...\n",
    "\n",
    "# # Function to process each row for a given prompt type\n",
    "# def process_row_for_prompt(data):\n",
    "#     row, prompt_type = data\n",
    "#     content = row['content'][:2048]\n",
    "#     cloze_prompt = get_prompt(prompt_type)  # Assuming this function exists in your full code\n",
    "#     prompt_content = f\"Please state a single probable answer if you are uncertain, lack evidence or unverified, but please answer with your single word: 'True' or 'False'.'  prompt: '{cloze_prompt}. Content: '{content}'\"\n",
    "\n",
    "#     url = 'https://api.deepinfra.com/v1/inference/tiiuae/falcon-40b-q51' #Falcon model url\n",
    "#     headers = {\n",
    "#         'Content-Type': 'application/json',\n",
    "#         'Authorization': f'Bearer {DEEPINFRA_TOKEN}'\n",
    "#     }\n",
    "#     data_payload = {\n",
    "#         'input': {\n",
    "#             'prompt': prompt_content\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     try:\n",
    "#         response = requests.post(url, headers=headers, json=data_payload)\n",
    "#         response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "\n",
    "#         json_response = response.json()\n",
    "#         model_output = json_response['choices'][0]['message']['content'].strip()\n",
    "#         return (row['uuid'], model_output, content, row['label'], prompt_type, row['pre_post_GPT'], row['article_type'], row['dataset_source'], row['source_type'])\n",
    "\n",
    "#     except requests.RequestException as e:\n",
    "#         error_message = f\"Error for uuid {row['uuid']} with prompt {prompt_type}: HTTP status {e}\"\n",
    "#         if response.text:\n",
    "#             error_message += f\", Response body: {response.text}\"\n",
    "#         print(error_message)\n",
    "#         return (row['uuid'], \"Error\", content, row['label'], prompt_type, row['pre_post_GPT'], row['article_type'], row['dataset_source'], row['source_type'])\n",
    "\n",
    "\n",
    "# # Parallelize using ThreadPoolExecutor\n",
    "# results = []\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     for data in tqdm(executor.map(process_row_for_prompt, all_data), total=len(all_data)):\n",
    "#         results.append(data)\n",
    "#         # Adjust the save condition (you might want to modify this based on the frequency of saving)\n",
    "#         if len(results) % 10 == 0 or len(results) == len(all_data):\n",
    "#             print(\"Saving progress.\")\n",
    "#             columns = ['uuid', 'Falcon2-label', 'content', 'label', 'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type']\n",
    "#             temp_df = pd.DataFrame(results, columns=columns)\n",
    "#             progress_df = pd.concat([progress_df, temp_df], axis=0, ignore_index=True)\n",
    "#             progress_df.to_csv(backup_path, mode='w', header=True, index=False)\n",
    "#             results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yffogoeidG6p"
   },
   "source": [
    "## Open AI\n",
    "\n",
    "We using Open AIAPI for inference with the following LLMs.\n",
    "\n",
    "\n",
    "1.   GPT3.5- Turbo\n",
    "2.   GPT4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9YOt8yFZJha"
   },
   "source": [
    "### GPT3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aIKFF3JYhXgn"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "B7lf1JAGxlUR"
   },
   "outputs": [],
   "source": [
    "OPENAI_TOKEN = os.getenv('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T234rV33hxxX",
    "outputId": "1057f563-274e-427b-d0d2-a720e279a294"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qSmma39cvStS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:13<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_TOKEN)\n",
    "\n",
    "# Function for AI text classification\n",
    "def ai_text_classification(prompt, content):\n",
    "    # openai.api_key = OPENAI_TOKEN\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            time.sleep(11)  # Sleep for 600 milliseconds\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=0.7,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt},\n",
    "                    {\"role\": \"user\", \"content\": content[:4097]},\n",
    "                ],\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(30)  # Wait for 60 seconds before retrying\n",
    "\n",
    "# Load the prompt pattern\n",
    "prompt_types = [\n",
    "    \"VaN\", \"A_CoT\", \"X_CoT\", \"A_Con\",\n",
    "    \"MSReN\", \"MSReN_paper\", \"MSReN_CoT\",\n",
    "    \"DeF_Gen\", \"DeF_SpeC\",\n",
    "    \"Analyze_Cld2\", \"Analyze_AI_GPT\"\n",
    "]\n",
    "\n",
    "# Ensure backup directory exists\n",
    "backup_folder = 'bkp_gpt3_5'\n",
    "os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "# Load previous progress if exists\n",
    "backup_path = os.path.join(backup_folder, \"progress.csv\")\n",
    "last_processed_uuid = None\n",
    "last_processed_prompt = None\n",
    "\n",
    "try:\n",
    "    progress_df = pd.read_csv(backup_path)\n",
    "    last_processed_uuid = progress_df.iloc[-1]['uuid']\n",
    "    last_processed_prompt = progress_df.iloc[-1]['Prompt_type']\n",
    "except FileNotFoundError:\n",
    "    progress_df = pd.DataFrame(columns=[\n",
    "        'uuid', 'GPT3_5T-label', 'content', 'label',\n",
    "        'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'\n",
    "    ])\n",
    "\n",
    "# Create a list of all combinations of rows and prompt types\n",
    "if last_processed_uuid and last_processed_prompt:\n",
    "    start_idx = df_gen[df_gen['uuid'] == last_processed_uuid].index[0]\n",
    "    start_prompt_idx = prompt_types.index(last_processed_prompt) + 1\n",
    "    if start_prompt_idx >= len(prompt_types):\n",
    "        start_idx += 1\n",
    "        start_prompt_idx = 0\n",
    "\n",
    "    all_data = [\n",
    "        (row, prompt)\n",
    "        for _, row in df_gen[:1].iloc[start_idx:].iterrows()\n",
    "        for prompt in (prompt_types[start_prompt_idx:]\n",
    "        if row['uuid'] == last_processed_uuid else prompt_types)\n",
    "    ]\n",
    "else:\n",
    "    all_data = [\n",
    "        (row, prompt)\n",
    "        for _, row in df_gen[:1].iterrows()\n",
    "        for prompt in prompt_types\n",
    "    ]\n",
    "\n",
    "# Function to process each row for a given prompt type\n",
    "def process_row_for_prompt(data):\n",
    "    row, prompt_type = data\n",
    "    content = row['content']\n",
    "    cloze_prompt = get_prompt(prompt_type)  # assuming this function exists in your full code\n",
    "    prompt_content = f\"Content: '{content}' prompt: '{cloze_prompt}. Please strictly state a single probable answer if you are uncertain, lack evidence or unverified, but please strictly answer with your single word: 'True' or 'False'.'\"\n",
    "\n",
    "    try:\n",
    "        model_output = ai_text_classification(prompt_content, content)\n",
    "        return (\n",
    "            row['uuid'], model_output, content, row['label'], prompt_type,\n",
    "            row['pre_post_GPT'], row['article_type'], row['dataset_source'],\n",
    "            row['source_type']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error for uuid {row['uuid']} with prompt {prompt_type}: {str(e)}\")\n",
    "        return (\n",
    "            row['uuid'], \"Error\", content, row['label'], prompt_type,\n",
    "            row['pre_post_GPT'], row['article_type'], row['dataset_source'],\n",
    "            row['source_type']\n",
    "        )\n",
    "\n",
    "# Parallel execution using ThreadPoolExecutor\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    for data in tqdm(executor.map(process_row_for_prompt, all_data), total=len(all_data)):\n",
    "        results.append(data)\n",
    "\n",
    "        # Adjust the save condition (you might want to modify this based on the frequency of saving)\n",
    "        if len(results) % 10 == 0 or len(results) == len(all_data):\n",
    "            print(\"Saving progress.\")\n",
    "            columns = [\n",
    "                'uuid', 'GPT3_5T-label', 'content', 'label',\n",
    "                'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'\n",
    "            ]\n",
    "            temp_df = pd.DataFrame(results, columns=columns)\n",
    "            progress_df = pd.concat([progress_df, temp_df], axis=0, ignore_index=True)\n",
    "            progress_df.to_csv(backup_path, mode='w', header=True, index=False)\n",
    "            results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Their prompt for GPT-3.5 is chat format and puts the text under discussion in twice, once in the system prompt and then again in the user prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQCtUtepQiQx"
   },
   "source": [
    "### GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n4rli_VIQiQx",
    "outputId": "361382d3-8d00-4228-e311-f0901f105250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.28.1\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cij9YeBjQiQx"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_Y17q1tQiQy"
   },
   "outputs": [],
   "source": [
    "api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edZmboe4QiQz",
    "outputId": "b69097da-4207-4a70-9c7a-b66870133566"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHuPa-bvQiQz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function for AI text classification\n",
    "def ai_text_classification(prompt, content):\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            time.sleep(11)  # Sleep for 11 seconds\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                temperature=0.7,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt},\n",
    "                    {\"role\": \"user\", \"content\": content[:4097]},\n",
    "                ],\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            print(error_str)\n",
    "            if \"Rate limit reached\" in error_str:\n",
    "                wait_time = re.search(r\"try again in (\\d+)ms\", error_str)\n",
    "                if wait_time:\n",
    "                    wait_time = int(wait_time.group(1))/1000  # Convert ms to seconds\n",
    "                    time.sleep(wait_time + 5)  # Adding an extra 5 seconds for buffer\n",
    "            else:\n",
    "                time.sleep(30)\n",
    "\n",
    "# Load the prompt pattern\n",
    "prompt_types = [\"VaN\", \"A_CoT\", \"X_CoT\", \"A_Con\", \"MSReN\", \"MSReN_CoT\", \"DeF_Gen\", \"DeF_SpeC\", \"Analyze_Cld2\", \"Analyze_AI_GPT\"]\n",
    "\n",
    "# Ensure backup directory exists\n",
    "backup_folder = \"GPT4\"\n",
    "os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "# Load previous progress if exists\n",
    "backup_path = os.path.join(backup_folder, \"progress.csv\")\n",
    "last_processed_uuid = None\n",
    "last_processed_prompt = None\n",
    "\n",
    "try:\n",
    "    progress_df = pd.read_csv(backup_path, encoding='latin1')\n",
    "    last_processed_uuid = progress_df.iloc[-1]['uuid']\n",
    "    last_processed_prompt = progress_df.iloc[-1]['Prompt_type']\n",
    "except FileNotFoundError:\n",
    "    progress_df = pd.DataFrame(columns=['uuid', 'GPT4-label', 'content', 'label', 'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'])\n",
    "\n",
    "# Create a list of all combinations of rows and prompt types\n",
    "if last_processed_uuid and last_processed_prompt:\n",
    "    start_idx = Experiment_data[Experiment_data['uuid'] == last_processed_uuid].index[0]\n",
    "    start_prompt_idx = prompt_types.index(last_processed_prompt) + 1\n",
    "    if start_prompt_idx >= len(prompt_types):\n",
    "        start_idx += 1\n",
    "        start_prompt_idx = 0\n",
    "\n",
    "    all_data = [(row, prompt) for _, row in Experiment_data.iloc[start_idx:].iterrows() for prompt in (prompt_types[start_prompt_idx:] if row['uuid'] == last_processed_uuid else prompt_types)]\n",
    "else:\n",
    "    all_data = [(row, prompt) for _, row in Experiment_data.iterrows() for prompt in prompt_types]\n",
    "\n",
    "# Function to process each row for a given prompt type\n",
    "def process_row_for_prompt(data):\n",
    "    row, prompt_type = data\n",
    "    content = row['content']\n",
    "    cloze_prompt = get_prompt(prompt_type)  # assuming this function exists in your full code\n",
    "    prompt_content = f\"Content: '{content}' prompt: '{cloze_prompt}. Please strictly state a single probable answer if you are uncertain, lack evidence or unverified, but please strictly answer with your single word: 'True' or 'False'.'\"\n",
    "\n",
    "    try:\n",
    "        model_output = ai_text_classification(prompt_content, content)\n",
    "        return (row['uuid'], model_output, content, row['label'], prompt_type, row['pre_post_GPT'], row['article_type'], row['dataset_source'], row['source_type'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error for uuid {row['uuid']} with prompt {prompt_type}: {str(e)}\")\n",
    "        return (row['uuid'], \"Error\", content, row['label'], prompt_type, row['pre_post_GPT'], row['article_type'], row['dataset_source'], row['source_type'])\n",
    "\n",
    "# Process each data sequentially\n",
    "results = []\n",
    "for data in tqdm(all_data, total=len(all_data)):\n",
    "    result = process_row_for_prompt(data)\n",
    "    results.append(result)\n",
    "\n",
    "    # Adjust the save condition (you might want to modify this based on the frequency of saving)\n",
    "    if len(results) % 10 == 0 or len(results) == len(all_data):\n",
    "        print(\"Saving progress.\")\n",
    "        columns = ['uuid', 'GPT4-label', 'content', 'label', 'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type']\n",
    "        temp_df = pd.DataFrame(results, columns=columns)\n",
    "        progress_df = pd.concat([progress_df, temp_df], axis=0, ignore_index=True)\n",
    "        progress_df.to_csv(backup_path, mode='w', header=True, index=False)\n",
    "        results = []\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PiIyvv44QGqr",
    "SiFahpJHvlDX",
    "mh8FZZraheyX",
    "ve7y5Y1TTwGG",
    "SMQmSgXG-FLF",
    "yffogoeidG6p",
    "g9YOt8yFZJha",
    "CQCtUtepQiQx"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "f3-kernel",
   "language": "python",
   "name": "f3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

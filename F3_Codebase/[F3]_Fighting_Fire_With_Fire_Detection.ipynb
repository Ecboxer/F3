{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiIyvv44QGqr"
   },
   "source": [
    "# Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aJD0ch3RQGqu"
   },
   "outputs": [],
   "source": [
    "# from google.colab import data_table\n",
    "# data_table.enable_dataframe_formatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lpx0GNJw4GBm",
    "outputId": "3906d330-6225-42f3-cdb3-55c1278c35ac"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5s_Zrhw081F",
    "outputId": "969e827e-7823-4337-8e63-2a97c5159d0d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiFahpJHvlDX"
   },
   "source": [
    "# Import Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j5OjqdCBZEnc"
   },
   "outputs": [],
   "source": [
    "# %cd /gdrive/My Drive/F3/ #REPLACE YOUR GOOGLE DRIVE DIRECTORY HERE\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27667, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>human_content</th>\n",
       "      <th>ai_content</th>\n",
       "      <th>model</th>\n",
       "      <th>num_completion_token</th>\n",
       "      <th>num_original_token</th>\n",
       "      <th>num_prompt_token</th>\n",
       "      <th>num_iagenerated_token</th>\n",
       "      <th>original_label</th>\n",
       "      <th>source_type</th>\n",
       "      <th>ai_generated_label</th>\n",
       "      <th>article_type</th>\n",
       "      <th>pre_post_GPT</th>\n",
       "      <th>dataset_source</th>\n",
       "      <th>Prompt_type</th>\n",
       "      <th>GPT_Entailment_Explanation</th>\n",
       "      <th>GPT_NLI_Label</th>\n",
       "      <th>FLAN_NLI_Label</th>\n",
       "      <th>BERTScore</th>\n",
       "      <th>BLEURT-20</th>\n",
       "      <th>aigenerated_content_cleaned</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>tokens_ai</th>\n",
       "      <th>tokens_human</th>\n",
       "      <th>diff_ai_to_human</th>\n",
       "      <th>diff_human_to_ai</th>\n",
       "      <th>Multicall_labels</th>\n",
       "      <th>AlignScore</th>\n",
       "      <th>human_TextBlob_polarity</th>\n",
       "      <th>human_TextBlob_subjectivity</th>\n",
       "      <th>ai_TextBlob_polarity</th>\n",
       "      <th>ai_TextBlob_subjectivity</th>\n",
       "      <th>human_embed</th>\n",
       "      <th>ai_embed</th>\n",
       "      <th>semantic_distance</th>\n",
       "      <th>edit_distance</th>\n",
       "      <th>jaccard_similarity</th>\n",
       "      <th>overlap_coefficient</th>\n",
       "      <th>combined_hue</th>\n",
       "      <th>Text-bison-NLI</th>\n",
       "      <th>PaLM_NLI</th>\n",
       "      <th>PaLM_categories</th>\n",
       "      <th>uuid.1</th>\n",
       "      <th>Llama2-Entailment</th>\n",
       "      <th>Llama_NLI</th>\n",
       "      <th>GPT_NLI_plus_ai_label</th>\n",
       "      <th>Llama_NLI_plus_ai_label</th>\n",
       "      <th>PaLM_NLI_plus_ai_label</th>\n",
       "      <th>Logical Consistency</th>\n",
       "      <th>factual_consistency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4ac69fef-1574-4a5e-8e85-dccdf2b56f69</td>\n",
       "      <td>Blake Masters: \"Of course, I support Lindsey G...</td>\n",
       "      <td>üö®BREAKINGüö® Blake Masters just declared his sup...</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>178</td>\n",
       "      <td>265</td>\n",
       "      <td>real</td>\n",
       "      <td>AI Machine</td>\n",
       "      <td>fake</td>\n",
       "      <td>twitter post</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>x-Gen</td>\n",
       "      <td>minor</td>\n",
       "      <td>non-entailment. The premise states that Blake ...</td>\n",
       "      <td>not-entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>0.955594</td>\n",
       "      <td>0.400188</td>\n",
       "      <td>blake masters just declared his support for l...</td>\n",
       "      <td>0</td>\n",
       "      <td>['Blake', 'Masters', 'just', 'declared', 'his'...</td>\n",
       "      <td>['Blake', 'Masters', ':', '``', 'Of', 'course'...</td>\n",
       "      <td>{'!', 'a', 'to', 'unacceptable', 'on', 'women'...</td>\n",
       "      <td>{\"'ll\", 'it', 'Of', 'the', 'third-trimester', ...</td>\n",
       "      <td>fake-AI</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>[[ 1.70015231e-01 -3.15304965e-01  9.92894396e...</td>\n",
       "      <td>[[ 1.91470772e-01 -2.50223905e-01  6.75175339e...</td>\n",
       "      <td>0.005712</td>\n",
       "      <td>181</td>\n",
       "      <td>0.164384</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>minor_fake</td>\n",
       "      <td>{'predictions': [{'citationMetadata': {'citati...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Finance, Health, Legal, Politics, Violent</td>\n",
       "      <td>4ac69fef-1574-4a5e-8e85-dccdf2b56f69</td>\n",
       "      <td>Entailment.\\n\\nThe statement by Blake Masters ...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>not-entailment_fake</td>\n",
       "      <td>entailment_fake</td>\n",
       "      <td>entailment_fake</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>consistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346688be-2b10-403d-a8ae-5fe9faed3214</td>\n",
       "      <td>‚ÄúThe Supreme Court has given us an opportunity...</td>\n",
       "      <td>BREAKING: Texas Governor announces new law to ...</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>67</td>\n",
       "      <td>57</td>\n",
       "      <td>201</td>\n",
       "      <td>268</td>\n",
       "      <td>real</td>\n",
       "      <td>AI Machine</td>\n",
       "      <td>fake</td>\n",
       "      <td>twitter post</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>x-Gen</td>\n",
       "      <td>minor</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>not-entailment</td>\n",
       "      <td>0.946922</td>\n",
       "      <td>0.060618</td>\n",
       "      <td>texas governor announces new law to completel...</td>\n",
       "      <td>0</td>\n",
       "      <td>[':', 'Texas', 'Governor', 'announces', 'new',...</td>\n",
       "      <td>['‚Äú', 'The', 'Supreme', 'Court', 'has', 'given...</td>\n",
       "      <td>{'announces', 'state', 'ban', 'a', 'to', 'of',...</td>\n",
       "      <td>{'lead', '‚Äù', 'given', 'must', 'opportunity', ...</td>\n",
       "      <td>fake-AI</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>-0.037500</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>-0.165909</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>[[ 1.75131306e-01 -3.00048381e-01  1.03785113e...</td>\n",
       "      <td>[[ 1.87339753e-01 -2.37771243e-01  8.31547678e...</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>249</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>minor_fake</td>\n",
       "      <td>{'predictions': [{'safetyAttributes': {'blocke...</td>\n",
       "      <td>not-entailment</td>\n",
       "      <td>Death, Harm &amp; Tragedy, Health, Legal, Politics...</td>\n",
       "      <td>346688be-2b10-403d-a8ae-5fe9faed3214</td>\n",
       "      <td>Entailment.\\n\\nThe premise states that the Tex...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment_fake</td>\n",
       "      <td>entailment_fake</td>\n",
       "      <td>not-entailment_fake</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>consistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8b95e37b-7396-4759-a828-1e128fa59471</td>\n",
       "      <td>Thinking about the many times Justices Gorsuch...</td>\n",
       "      <td>üö®BREAKING NEWSüö® Justices Gorsuch, Kavanaugh, a...</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>71</td>\n",
       "      <td>44</td>\n",
       "      <td>183</td>\n",
       "      <td>254</td>\n",
       "      <td>real</td>\n",
       "      <td>AI Machine</td>\n",
       "      <td>fake</td>\n",
       "      <td>twitter post</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>x-Gen</td>\n",
       "      <td>minor</td>\n",
       "      <td>There is no logical entailment between the pre...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>not-entailment</td>\n",
       "      <td>0.964400</td>\n",
       "      <td>0.315208</td>\n",
       "      <td>news justices gorsuch kavanaugh and barrett w...</td>\n",
       "      <td>0</td>\n",
       "      <td>['NEWS', 'Justices', 'Gorsuch', ',', 'Kavanaug...</td>\n",
       "      <td>['Thinking', 'about', 'the', 'many', 'times', ...</td>\n",
       "      <td>{'!', 'held', 'lied', 'to', 'must', 'overturn'...</td>\n",
       "      <td>{'‚Äù', 'it', 'established', 'precedent.', 'many...</td>\n",
       "      <td>fake-AI</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[ 1.33914545e-01 -2.83469945e-01  1.20100819e...</td>\n",
       "      <td>[[ 1.54923260e-01 -2.36947343e-01  9.40273628e...</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>206</td>\n",
       "      <td>0.084337</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>minor_fake</td>\n",
       "      <td>{'predictions': [{'citationMetadata': {'citati...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Legal, Politics, Public Safety, Violent</td>\n",
       "      <td>8b95e37b-7396-4759-a828-1e128fa59471</td>\n",
       "      <td>Not Entailment.\\n\\nThe premise states that the...</td>\n",
       "      <td>not-entailment</td>\n",
       "      <td>entailment_fake</td>\n",
       "      <td>not-entailment_fake</td>\n",
       "      <td>entailment_fake</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>consistent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid  \\\n",
       "0  4ac69fef-1574-4a5e-8e85-dccdf2b56f69   \n",
       "1  346688be-2b10-403d-a8ae-5fe9faed3214   \n",
       "2  8b95e37b-7396-4759-a828-1e128fa59471   \n",
       "\n",
       "                                       human_content  \\\n",
       "0  Blake Masters: \"Of course, I support Lindsey G...   \n",
       "1  ‚ÄúThe Supreme Court has given us an opportunity...   \n",
       "2  Thinking about the many times Justices Gorsuch...   \n",
       "\n",
       "                                          ai_content               model  \\\n",
       "0  üö®BREAKINGüö® Blake Masters just declared his sup...  gpt-3.5-turbo-0301   \n",
       "1  BREAKING: Texas Governor announces new law to ...  gpt-3.5-turbo-0301   \n",
       "2  üö®BREAKING NEWSüö® Justices Gorsuch, Kavanaugh, a...  gpt-3.5-turbo-0301   \n",
       "\n",
       "   num_completion_token  num_original_token  num_prompt_token  \\\n",
       "0                    87                  39               178   \n",
       "1                    67                  57               201   \n",
       "2                    71                  44               183   \n",
       "\n",
       "   num_iagenerated_token original_label source_type ai_generated_label  \\\n",
       "0                    265           real  AI Machine               fake   \n",
       "1                    268           real  AI Machine               fake   \n",
       "2                    254           real  AI Machine               fake   \n",
       "\n",
       "   article_type pre_post_GPT dataset_source Prompt_type  \\\n",
       "0  twitter post      pre-GPT          x-Gen       minor   \n",
       "1  twitter post      pre-GPT          x-Gen       minor   \n",
       "2  twitter post      pre-GPT          x-Gen       minor   \n",
       "\n",
       "                          GPT_Entailment_Explanation   GPT_NLI_Label  \\\n",
       "0  non-entailment. The premise states that Blake ...  not-entailment   \n",
       "1                                         entailment      entailment   \n",
       "2  There is no logical entailment between the pre...      entailment   \n",
       "\n",
       "   FLAN_NLI_Label  BERTScore  BLEURT-20  \\\n",
       "0      entailment   0.955594   0.400188   \n",
       "1  not-entailment   0.946922   0.060618   \n",
       "2  not-entailment   0.964400   0.315208   \n",
       "\n",
       "                         aigenerated_content_cleaned  emoji_count  \\\n",
       "0   blake masters just declared his support for l...            0   \n",
       "1   texas governor announces new law to completel...            0   \n",
       "2   news justices gorsuch kavanaugh and barrett w...            0   \n",
       "\n",
       "                                           tokens_ai  \\\n",
       "0  ['Blake', 'Masters', 'just', 'declared', 'his'...   \n",
       "1  [':', 'Texas', 'Governor', 'announces', 'new',...   \n",
       "2  ['NEWS', 'Justices', 'Gorsuch', ',', 'Kavanaug...   \n",
       "\n",
       "                                        tokens_human  \\\n",
       "0  ['Blake', 'Masters', ':', '``', 'Of', 'course'...   \n",
       "1  ['‚Äú', 'The', 'Supreme', 'Court', 'has', 'given...   \n",
       "2  ['Thinking', 'about', 'the', 'many', 'times', ...   \n",
       "\n",
       "                                    diff_ai_to_human  \\\n",
       "0  {'!', 'a', 'to', 'unacceptable', 'on', 'women'...   \n",
       "1  {'announces', 'state', 'ban', 'a', 'to', 'of',...   \n",
       "2  {'!', 'held', 'lied', 'to', 'must', 'overturn'...   \n",
       "\n",
       "                                    diff_human_to_ai Multicall_labels  \\\n",
       "0  {\"'ll\", 'it', 'Of', 'the', 'third-trimester', ...          fake-AI   \n",
       "1  {'lead', '‚Äù', 'given', 'must', 'opportunity', ...          fake-AI   \n",
       "2  {'‚Äù', 'it', 'established', 'precedent.', 'many...          fake-AI   \n",
       "\n",
       "   AlignScore  human_TextBlob_polarity  human_TextBlob_subjectivity  \\\n",
       "0    0.002663                -0.166667                     0.166667   \n",
       "1    0.003862                -0.037500                     0.491667   \n",
       "2    0.000981                 0.250000                     0.250000   \n",
       "\n",
       "   ai_TextBlob_polarity  ai_TextBlob_subjectivity  \\\n",
       "0             -0.450000                  0.700000   \n",
       "1             -0.165909                  0.463636   \n",
       "2              0.000000                  0.000000   \n",
       "\n",
       "                                         human_embed  \\\n",
       "0  [[ 1.70015231e-01 -3.15304965e-01  9.92894396e...   \n",
       "1  [[ 1.75131306e-01 -3.00048381e-01  1.03785113e...   \n",
       "2  [[ 1.33914545e-01 -2.83469945e-01  1.20100819e...   \n",
       "\n",
       "                                            ai_embed  semantic_distance  \\\n",
       "0  [[ 1.91470772e-01 -2.50223905e-01  6.75175339e...           0.005712   \n",
       "1  [[ 1.87339753e-01 -2.37771243e-01  8.31547678e...           0.004875   \n",
       "2  [[ 1.54923260e-01 -2.36947343e-01  9.40273628e...           0.005833   \n",
       "\n",
       "   edit_distance  jaccard_similarity  overlap_coefficient combined_hue  \\\n",
       "0            181            0.164384             0.315789   minor_fake   \n",
       "1            249            0.063158             0.150000   minor_fake   \n",
       "2            206            0.084337             0.162791   minor_fake   \n",
       "\n",
       "                                      Text-bison-NLI        PaLM_NLI  \\\n",
       "0  {'predictions': [{'citationMetadata': {'citati...      entailment   \n",
       "1  {'predictions': [{'safetyAttributes': {'blocke...  not-entailment   \n",
       "2  {'predictions': [{'citationMetadata': {'citati...      entailment   \n",
       "\n",
       "                                     PaLM_categories  \\\n",
       "0          Finance, Health, Legal, Politics, Violent   \n",
       "1  Death, Harm & Tragedy, Health, Legal, Politics...   \n",
       "2            Legal, Politics, Public Safety, Violent   \n",
       "\n",
       "                                 uuid.1  \\\n",
       "0  4ac69fef-1574-4a5e-8e85-dccdf2b56f69   \n",
       "1  346688be-2b10-403d-a8ae-5fe9faed3214   \n",
       "2  8b95e37b-7396-4759-a828-1e128fa59471   \n",
       "\n",
       "                                   Llama2-Entailment       Llama_NLI  \\\n",
       "0  Entailment.\\n\\nThe statement by Blake Masters ...      entailment   \n",
       "1  Entailment.\\n\\nThe premise states that the Tex...      entailment   \n",
       "2  Not Entailment.\\n\\nThe premise states that the...  not-entailment   \n",
       "\n",
       "  GPT_NLI_plus_ai_label Llama_NLI_plus_ai_label PaLM_NLI_plus_ai_label  \\\n",
       "0   not-entailment_fake         entailment_fake        entailment_fake   \n",
       "1       entailment_fake         entailment_fake    not-entailment_fake   \n",
       "2       entailment_fake     not-entailment_fake        entailment_fake   \n",
       "\n",
       "  Logical Consistency factual_consistency  \n",
       "0        inconsistent          consistent  \n",
       "1        inconsistent          consistent  \n",
       "2        inconsistent          consistent  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../F3_Dataset/Full Clean Dataset/F3_Consistency.csv')\n",
    "print(df.shape)\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 55334 entries, 0 to 27666\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   uuid            55334 non-null  object\n",
      " 1   label           55334 non-null  object\n",
      " 2   article_type    55334 non-null  object\n",
      " 3   source_type     55334 non-null  object\n",
      " 4   pre_post_GPT    55334 non-null  object\n",
      " 5   dataset_source  55334 non-null  object\n",
      " 6   Prompt_type     55334 non-null  object\n",
      " 7   content         55334 non-null  object\n",
      " 8   text_length     55334 non-null  int64 \n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Create version with just the pre-scoring columns\n",
    "cols_shared = [\n",
    "    'uuid', 'label', 'article_type', 'source_type', 'pre_post_GPT',\n",
    "    'dataset_source', 'Prompt_type',\n",
    "]\n",
    "df_gen_wide = df.rename(columns={\n",
    "    'ai_generated_label': 'label',\n",
    "})[cols_shared + [\n",
    "    'human_content', 'ai_content',  # -> content\n",
    "    'num_original_token', 'num_completion_token',  # -> text_length\n",
    "]]\n",
    "\n",
    "# Human text\n",
    "df_gen_human = df_gen_wide.rename(columns={\n",
    "    'human_content': 'content',\n",
    "    'num_original_token': 'text_length',\n",
    "})[cols_shared + ['content', 'text_length']]\n",
    "df_gen_human.loc[:,'source_type'] = 'human'\n",
    "\n",
    "# LLM/AI text\n",
    "df_gen_llm = df_gen_wide.rename(columns={\n",
    "    'ai_content': 'content',\n",
    "    'num_completion_token': 'text_length',\n",
    "})[cols_shared + ['content', 'text_length']]\n",
    "df_gen_llm.loc[:,'source_type'] = 'LLM'\n",
    "\n",
    "df_gen = pd.concat([df_gen_human, df_gen_llm])\n",
    "df_gen.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5534 entries, 5134 to 1165\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   uuid            5534 non-null   object\n",
      " 1   label           5534 non-null   object\n",
      " 2   article_type    5534 non-null   object\n",
      " 3   source_type     5534 non-null   object\n",
      " 4   pre_post_GPT    5534 non-null   object\n",
      " 5   dataset_source  5534 non-null   object\n",
      " 6   Prompt_type     5534 non-null   object\n",
      " 7   content         5534 non-null   object\n",
      " 8   text_length     5534 non-null   int64 \n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 432.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create subsetted version for replication\n",
    "\n",
    "# Create version with just the pre-scoring columns\n",
    "cols_shared = [\n",
    "    'uuid', 'label', 'article_type', 'source_type', 'pre_post_GPT',\n",
    "    'dataset_source', 'Prompt_type',\n",
    "]\n",
    "df_gen_wide = df.rename(columns={\n",
    "    'ai_generated_label': 'label',\n",
    "})[cols_shared + [\n",
    "    'human_content', 'ai_content',  # -> content\n",
    "    'num_original_token', 'num_completion_token',  # -> text_length\n",
    "]]\n",
    "\n",
    "df_gen_wide_ss = df_gen_wide.sample(frac=.1, random_state=0)\n",
    "\n",
    "# Human text\n",
    "df_gen_human_ss = df_gen_wide_ss.rename(columns={\n",
    "    'human_content': 'content',\n",
    "    'num_original_token': 'text_length',\n",
    "})[cols_shared + ['content', 'text_length']]\n",
    "df_gen_human_ss.loc[:,'source_type'] = 'human'\n",
    "\n",
    "# LLM/AI text\n",
    "df_gen_llm_ss = df_gen_wide_ss.rename(columns={\n",
    "    'ai_content': 'content',\n",
    "    'num_completion_token': 'text_length',\n",
    "})[cols_shared + ['content', 'text_length']]\n",
    "df_gen_llm_ss.loc[:,'source_type'] = 'LLM'\n",
    "\n",
    "df_gen_ss = pd.concat([df_gen_human_ss, df_gen_llm_ss])\n",
    "df_gen_ss.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>label</th>\n",
       "      <th>article_type</th>\n",
       "      <th>source_type</th>\n",
       "      <th>pre_post_GPT</th>\n",
       "      <th>dataset_source</th>\n",
       "      <th>Prompt_type</th>\n",
       "      <th>content</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4ac69fef-1574-4a5e-8e85-dccdf2b56f69</td>\n",
       "      <td>fake</td>\n",
       "      <td>twitter post</td>\n",
       "      <td>human</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>x-Gen</td>\n",
       "      <td>minor</td>\n",
       "      <td>Blake Masters: \"Of course, I support Lindsey G...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346688be-2b10-403d-a8ae-5fe9faed3214</td>\n",
       "      <td>fake</td>\n",
       "      <td>twitter post</td>\n",
       "      <td>human</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>x-Gen</td>\n",
       "      <td>minor</td>\n",
       "      <td>‚ÄúThe Supreme Court has given us an opportunity...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8b95e37b-7396-4759-a828-1e128fa59471</td>\n",
       "      <td>fake</td>\n",
       "      <td>twitter post</td>\n",
       "      <td>human</td>\n",
       "      <td>pre-GPT</td>\n",
       "      <td>x-Gen</td>\n",
       "      <td>minor</td>\n",
       "      <td>Thinking about the many times Justices Gorsuch...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid label  article_type source_type  \\\n",
       "0  4ac69fef-1574-4a5e-8e85-dccdf2b56f69  fake  twitter post       human   \n",
       "1  346688be-2b10-403d-a8ae-5fe9faed3214  fake  twitter post       human   \n",
       "2  8b95e37b-7396-4759-a828-1e128fa59471  fake  twitter post       human   \n",
       "\n",
       "  pre_post_GPT dataset_source Prompt_type  \\\n",
       "0      pre-GPT          x-Gen       minor   \n",
       "1      pre-GPT          x-Gen       minor   \n",
       "2      pre-GPT          x-Gen       minor   \n",
       "\n",
       "                                             content  text_length  \n",
       "0  Blake Masters: \"Of course, I support Lindsey G...           39  \n",
       "1  ‚ÄúThe Supreme Court has given us an opportunity...           57  \n",
       "2  Thinking about the many times Justices Gorsuch...           44  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gen[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_type\n",
       "human    27667\n",
       "LLM      27667\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gen['source_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_type\n",
       "human    2767\n",
       "LLM      2767\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gen_ss['source_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAfyFVZjlEw9"
   },
   "source": [
    "# Classification Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mh8FZZraheyX"
   },
   "source": [
    "## Cloze Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DjvrQjdUCYAW"
   },
   "outputs": [],
   "source": [
    "\"\"\"Generate a prompt using template.\n",
    "\n",
    "  Args:\n",
    "    F3 prompt template (Œ∏) has three parameters: (1) content (C) embeds data to be analyzed. The content parameter in the the function [ai_text_generator] (2) Impersonator (R)\n",
    "    establishes context, guides LLMs‚Äô generation and detection, and overrides alignment-tuning. (3) Instructor (I)\n",
    "    provides directives to guide LLM.\n",
    "\n",
    "  Returns:\n",
    "    A generated prompt.\n",
    "\"\"\"\n",
    "def get_prompt(prompt_type):\n",
    "\n",
    "  impersonator = \"\"\n",
    "  instructor = \"\"\n",
    "  cloze_prompt = \"Is this real or fake news? {_____}\"\n",
    "\n",
    "  if prompt_type == \"VaN\":# Technique\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news.\"\n",
    "    instructor = \"Analyze the given text and determine if it is real or fake news.\"\n",
    "\n",
    "  elif prompt_type == \"A_CoT\":# Technique  # Z_CoT\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news.\"\n",
    "    instructor = \"Analyze the given text, think step-by-step, and determine if it is real or fake news.\"\n",
    "\n",
    "  elif prompt_type == \"X_CoT\":# Technique\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news.\"\n",
    "    instructor = \"Analyze the given text, explain your reasoning step-by-step, and determine if it is real or fake news.\"\n",
    "\n",
    "  elif prompt_type == \"A_Con\":# Technique\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news with confidence estimates.\"\n",
    "    instructor = \"Analyze the given text, provide a confidence score between 0-100%, and determine if it is real or fake news.\"\n",
    "\n",
    "  elif prompt_type == \"MSReN\":# Technique\n",
    "    impersonator = \"You are an AI fact checker trained to detect fake news.\"\n",
    "    instructor = \"Analyze the text in detail as a fact checker would. Check for misleading info, false claims, biased language. If real, respond 'True', if fake, respond 'False'.\"\n",
    "  \n",
    "  # From paper\n",
    "  elif prompt_type == \"MSReN_paper\":# Technique\n",
    "    impersonator = \"You are an AI fact checker trained to detect fake news.\"\n",
    "    instructor = \"Analyze the text in detail as a fact checker would solve it by splitting your reasoning into steps. Check for misleading info, false claims, biased language. If real, respond ‚ÄôTrue‚Äô, if fake, respond ‚ÄôFalse‚Äô.\"\n",
    "  \n",
    "  elif prompt_type == \"MSReN_CoT\":# Technique\n",
    "    impersonator = \"You are an AI fact checker trained to detect fake news.\"\n",
    "    instructor = \"Analyze the text in detail as a fact checker would. Explain your reasoning step-by-step. If real, respond 'True', if fake, respond 'False'.\"\n",
    "\n",
    "  elif prompt_type == \"DeF_Gen\":# Technique\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news.\"\n",
    "    instructor = \"Determine if the text contains factual information supported by evidence (real) or misleading/inaccurate information (fake). Respond with 'True' or 'False'.\"\n",
    "\n",
    "  elif prompt_type == \"DeF_SpeC\":# Technique\n",
    "    impersonator = \"You are an AI assistant trained to detect fake news.\"\n",
    "    instructor = \"Analyze the tone, language, sources to determine if the text is real (supported by facts) or fake (misleading info). Respond with 'True' or 'False'.\"\n",
    "\n",
    "  elif prompt_type == \"Analyze_Cld2\":# Technique\n",
    "    impersonator = \"You are an AI assistant specially trained to detect fake news with high accuracy.\"\n",
    "    instructor = \"\"\"Analyze the given news article in depth. Check for the following indicators that it may be fake news:\n",
    "    - Inaccurate or misleading information\n",
    "    - Lack of evidence for claims\n",
    "    - Emotionally charged language with intent to provoke outrage or shock\n",
    "    - Biased portrayal of events or people\n",
    "    - Unverified sources or 'experts'\n",
    "    - Logical fallacies or conspiracy theories without proof\n",
    "\n",
    "    If the article exhibits multiple indicators of fake news, respond 'False'. If the article is factual and supported by credible evidence, respond 'True'. Your judgement should be highly accurate.\n",
    "    \"\"\"\n",
    "\n",
    "  elif prompt_type == \"Analyze_AI_GPT\":# Technique\n",
    "    impersonator = \"You are an AI trained with extensive knowledge up to 2022 on various news articles, both real and fake.\"\n",
    "    instructor = \"\"\"Analyze the given text for potential indicators of fake news, such as:\n",
    "    - Sensationalist or emotionally charged language.\n",
    "    - Absence of specific details or dates.\n",
    "    - Over-generalizations or sweeping statements.\n",
    "    - Statements that are too good to be true or overly dramatic.\n",
    "    - Lack of logical flow in arguments or jumping to conclusions without evidence.\n",
    "\n",
    "    It's essential to understand that without real-time verification capabilities, your judgment will be based on patterns and knowledge up to your last training. Using these textual cues and your training, determine the credibility of the given text. If it seems factual and consistent with your training, respond 'True'. If it exhibits patterns typical of fake news, respond 'False'.\"\"\"\n",
    "\n",
    "  else:\n",
    "    raise ValueError('Unexpected prompt_type:', prompt_type)\n",
    "    \n",
    "  prompt = f\"{impersonator} {instructor} {cloze_prompt}\"\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms3R06B6RKU6"
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve7y5Y1TTwGG"
   },
   "source": [
    "## DeepInfra\n",
    "\n",
    "\n",
    "We using [DeepInfra](https://deepinfra.com/) API for inference with the following LLMs.\n",
    "\n",
    "\n",
    "1.   LLaMa-2: meta-llama/Llama-2-70b-chat-hf\n",
    "2.   Dolly-2: databricks/dolly-v2-12b\n",
    "3.   Falcon: tiiuae/falcon-40b-q51\n",
    "4.   LLaMa-2-GPT: jondurbin/airoboros-l2-70b-gpt4-1.4.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMQmSgXG-FLF"
   },
   "source": [
    "### TODO Llama-2 70b, Dolly-2, LLaMA-GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "# import openai\n",
    "# from openai import AsyncOpenAI\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "# import concurrent.futures\n",
    "import nest_asyncio  # To run asyncio in Jupyter notebooks\n",
    "import asyncio\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "HF_API_TOKEN = os.getenv('HF_KEY')\n",
    "client = InferenceClient(api_key=HF_API_TOKEN)\n",
    "\n",
    "# Set the prompt pattern\n",
    "prompt_types = [\n",
    "    \"VaN\", \"A_CoT\", \"X_CoT\", \"A_Con\",\n",
    "    \"MSReN\", \"MSReN_paper\", \"MSReN_CoT\",\n",
    "    \"DeF_Gen\", \"DeF_SpeC\",\n",
    "    \"Analyze_Cld2\", \"Analyze_AI_GPT\"\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Replace the model name below with:\n",
    "1.   LLaMa-2: meta-llama/Llama-2-70b-chat-hf             # TODO\n",
    "2.   Dolly-2: databricks/dolly-v2-12b                    # TODO\n",
    "3.   LLaMa-2-GPT: jondurbin/airoboros-l2-70b-gpt4-1.4.1  # TODO\n",
    "\"\"\"\n",
    "model_name = 'llama2'\n",
    "model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "api_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe capital of France is Paris.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 403 - {\"error\":\"The model jondurbin/airoboros-l2-7b-gpt4-1.4.1 is too large to be loaded automatically (13GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "HF_API_TOKEN = os.getenv(\"HF_KEY\")\n",
    "headers = {\"Authorization\": f\"Bearer {HF_API_TOKEN}\"}\n",
    "api_url = \"https://api-inference.huggingface.co/models/jondurbin/airoboros-l2-7b-gpt4-1.4.1\"\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 50,\n",
    "    }\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "response = requests.post(api_url, headers=headers, json=payload)\n",
    "end_time = time.time()\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"Response time: {end_time - start_time:.2f} seconds\")\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5534, 9)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gen_ss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutputMessage(role='assistant', content='The capital of France is Paris.', tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qq5Io42G5Uu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  18%|‚ñà‚ñä        | 2/11 [00:00<00:00, 12.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n",
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n",
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  36%|‚ñà‚ñà‚ñà‚ñã      | 4/11 [00:00<00:00, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n",
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n",
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 6/11 [00:00<00:00, 12.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n",
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 8/11 [00:04<00:02,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 9/11 [00:06<00:02,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 10/11 [00:07<00:01,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1, sending request for UUID 00ac2a98-33dd-4cce-befe-20d7b90a8c27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:09<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "# import openai\n",
    "# from openai import AsyncOpenAI\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "# import concurrent.futures\n",
    "import nest_asyncio  # To run asyncio in Jupyter notebooks\n",
    "import asyncio\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "HF_API_TOKEN = os.getenv('HF_KEY')\n",
    "client = InferenceClient(api_key=HF_API_TOKEN)\n",
    "\n",
    "# Set the prompt pattern\n",
    "prompt_types = [\n",
    "    \"VaN\", \"A_CoT\", \"X_CoT\", \"A_Con\",\n",
    "    \"MSReN\", \"MSReN_paper\", \"MSReN_CoT\",\n",
    "    \"DeF_Gen\", \"DeF_SpeC\",\n",
    "    \"Analyze_Cld2\", \"Analyze_AI_GPT\"\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Replace the model name below with:\n",
    "# Note: Llama-2-70b unavailable on HuggingFace\n",
    "1.   LLaMa-2: meta-llama/Llama-2-7b-chat-hf              # TODO\n",
    "2.   Dolly-2: databricks/dolly-v2-12b                    # TODO\n",
    "3.   LLaMa-2-GPT: jondurbin/airoboros-l2-70b-gpt4-1.4.1  # TODO\n",
    "4.   (New) Mixtral: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "\"\"\"\n",
    "model_name = 'llama2'\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "api_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "\n",
    "seeds = [0]\n",
    "for seed in seeds:\n",
    "    # Ensure backup directory exists\n",
    "    backup_folder = f'bkp_{model_name}_seed_{seed}'\n",
    "    os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "    # Load previous progress if exists\n",
    "    backup_path = os.path.join(backup_folder, \"progress.csv\")\n",
    "    last_processed_uuid = None\n",
    "    last_processed_prompt = None\n",
    "\n",
    "    try:\n",
    "        progress_df = pd.read_csv(backup_path)\n",
    "        last_processed_uuid = progress_df.iloc[-1]['uuid']\n",
    "        last_processed_prompt = progress_df.iloc[-1]['Prompt_type']\n",
    "    except FileNotFoundError:\n",
    "        progress_df = pd.DataFrame(columns=[\n",
    "            'uuid', f'{model_name}_label', 'content', 'label',\n",
    "            'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'\n",
    "        ])\n",
    "\n",
    "    # Create a list of all combinations of rows and prompt types, using progress_df if found\n",
    "    if last_processed_uuid and last_processed_prompt:\n",
    "        start_idx = df_gen_ss[df_gen_ss['uuid'] == last_processed_uuid].index[0]\n",
    "        start_prompt_idx = prompt_types.index(last_processed_prompt) + 1\n",
    "        if start_prompt_idx >= len(prompt_types):\n",
    "            start_idx += 1\n",
    "            start_prompt_idx = 0\n",
    "\n",
    "        all_data = [\n",
    "            (row, prompt)\n",
    "            for _, row in df_gen_ss[:1].iloc[start_idx:].iterrows()\n",
    "            for prompt in (prompt_types[start_prompt_idx:]\n",
    "            if row['uuid'] == last_processed_uuid else prompt_types)\n",
    "        ]\n",
    "    else:\n",
    "        all_data = [\n",
    "            (row, prompt)\n",
    "            for _, row in df_gen_ss[:1].iterrows()\n",
    "            for prompt in prompt_types\n",
    "        ]\n",
    "\n",
    "    # Function to process each row for a given prompt type\n",
    "    def process_row_for_prompt(\n",
    "        data, model_id: str, retries: int = 3, delay: float = 2., seed: int = None,\n",
    "    ):\n",
    "        row, prompt_type = data\n",
    "        content = row['content']\n",
    "        cloze_prompt = get_prompt(prompt_type)\n",
    "        \n",
    "        # Amended version without {______} in prompt\n",
    "        prompt_content = f\"Content: '{content}' prompt: '{cloze_prompt}. Please state a single probable answer if you are uncertain, lack evidence or unverified, but please answer with your single word: 'True' or 'False'.'\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_content,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                print(f\"Attempt {attempt + 1}, sending request for UUID {row['uuid']}\")\n",
    "\n",
    "                # # Text generation API request\n",
    "                # response = client.text_generation(\n",
    "                #     model=model_id,\n",
    "                #     prompt=prompt_content,\n",
    "                #     max_new_tokens=50,\n",
    "                #     seed=seed,\n",
    "                # )\n",
    "                # model_output = response.\n",
    "\n",
    "                # Chat completion API request\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=messages, \n",
    "                    max_tokens=100,  # Just expect True/False output\n",
    "                )\n",
    "                model_output = completion.choices[0].message\n",
    "\n",
    "                return (\n",
    "                    row['uuid'],\n",
    "                    model_output.content.strip(),\n",
    "                    content,\n",
    "                    row['label'],\n",
    "                    prompt_type,\n",
    "                    row['pre_post_GPT'],\n",
    "                    row['article_type'],\n",
    "                    row['dataset_source'],\n",
    "                    row['source_type']\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error for UUID {row['uuid']}: {e}\")\n",
    "                if hasattr(e, \"response\"):\n",
    "                    print(f\"Response details: {json.dumps(e.response.text, indent=2)}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    print(f\"Max retries reached for UUID {row['uuid']}\")\n",
    "                    return \"Error\"\n",
    "\n",
    "    # Process all data\n",
    "    results = []\n",
    "    for data in tqdm(all_data, total=len(all_data), desc=f'Seed {seed}'):\n",
    "        results.append(process_row_for_prompt(data, model_id))\n",
    "\n",
    "        # Adjust the save condition (you might want to modify this based on the frequency of saving)\n",
    "        if len(results) % 100 == 0 or len(results) == len(all_data):\n",
    "            print(\"Saving progress.\")\n",
    "            columns = [\n",
    "                'uuid', f'{model_name}_label', 'content', 'label',\n",
    "                'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'\n",
    "            ]\n",
    "            temp_df = pd.DataFrame(results, columns=columns)\n",
    "            progress_df = pd.concat([progress_df, temp_df], axis=0, ignore_index=True)\n",
    "            progress_df.to_csv(backup_path, mode='w', header=True, index=False)\n",
    "\n",
    "    # Save results after processing\n",
    "    columns = [\n",
    "        'uuid', f'{model_name}_label', 'content', 'label',\n",
    "        'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'\n",
    "    ]\n",
    "    temp_df = pd.DataFrame(results, columns=columns)\n",
    "    progress_df = pd.concat([progress_df, temp_df], axis=0, ignore_index=True)\n",
    "    progress_df.to_csv(backup_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCaNkpHMSHBG"
   },
   "source": [
    "### TODO Falcon-40b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKAGTphqSGnd"
   },
   "outputs": [],
   "source": [
    "# !pip install requests\n",
    "\n",
    "# Falcon is accessible via python request package below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "I3HFM0l1SYLa"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# import requests\n",
    "# from tqdm import tqdm\n",
    "# import concurrent.futures\n",
    "\n",
    "# # Set the prompt pattern\n",
    "# prompt_types = [\"VaN\", \"A_CoT\", \"X_CoT\", \"A_Con\", \"MSReN\", \"MSReN_CoT\", \"DeF_Gen\", \"DeF_SpeC\", \"Analyze_Cld2\", \"Analyze_AI_GPT\"]\n",
    "\n",
    "# # Token for authentication\n",
    "# DEEPINFRA_TOKEN = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "# # Ensure backup directory exists\n",
    "# backup_folder = \"Falcon2\"\n",
    "# os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "# # Load previous progress if exists\n",
    "# backup_path = os.path.join(backup_folder, \"progress.csv\")\n",
    "# last_processed_uuid = None\n",
    "# last_processed_prompt = None\n",
    "\n",
    "# try:\n",
    "#     progress_df = pd.read_csv(backup_path)\n",
    "#     last_processed_uuid = progress_df.iloc[-1]['uuid']\n",
    "#     last_processed_prompt = progress_df.iloc[-1]['Prompt_type']\n",
    "# except FileNotFoundError:\n",
    "#     progress_df = pd.DataFrame(columns=['uuid', 'Falcon2-label', 'content', 'label', 'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'])\n",
    "\n",
    "# # Create a list of all combinations of rows and prompt types\n",
    "# # ... [existing code for this part, which wasn't provided in your snippets] ...\n",
    "\n",
    "# # Function to process each row for a given prompt type\n",
    "# def process_row_for_prompt(data):\n",
    "#     row, prompt_type = data\n",
    "#     content = row['content'][:2048]\n",
    "#     cloze_prompt = get_prompt(prompt_type)  # Assuming this function exists in your full code\n",
    "#     prompt_content = f\"Please state a single probable answer if you are uncertain, lack evidence or unverified, but please answer with your single word: 'True' or 'False'.'  prompt: '{cloze_prompt}. Content: '{content}'\"\n",
    "\n",
    "#     url = 'https://api.deepinfra.com/v1/inference/tiiuae/falcon-40b-q51' #Falcon model url\n",
    "#     headers = {\n",
    "#         'Content-Type': 'application/json',\n",
    "#         'Authorization': f'Bearer {DEEPINFRA_TOKEN}'\n",
    "#     }\n",
    "#     data_payload = {\n",
    "#         'input': {\n",
    "#             'prompt': prompt_content\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     try:\n",
    "#         response = requests.post(url, headers=headers, json=data_payload)\n",
    "#         response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "\n",
    "#         json_response = response.json()\n",
    "#         model_output = json_response['choices'][0]['message']['content'].strip()\n",
    "#         return (row['uuid'], model_output, content, row['label'], prompt_type, row['pre_post_GPT'], row['article_type'], row['dataset_source'], row['source_type'])\n",
    "\n",
    "#     except requests.RequestException as e:\n",
    "#         error_message = f\"Error for uuid {row['uuid']} with prompt {prompt_type}: HTTP status {e}\"\n",
    "#         if response.text:\n",
    "#             error_message += f\", Response body: {response.text}\"\n",
    "#         print(error_message)\n",
    "#         return (row['uuid'], \"Error\", content, row['label'], prompt_type, row['pre_post_GPT'], row['article_type'], row['dataset_source'], row['source_type'])\n",
    "\n",
    "\n",
    "# # Parallelize using ThreadPoolExecutor\n",
    "# results = []\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     for data in tqdm(executor.map(process_row_for_prompt, all_data), total=len(all_data)):\n",
    "#         results.append(data)\n",
    "#         # Adjust the save condition (you might want to modify this based on the frequency of saving)\n",
    "#         if len(results) % 10 == 0 or len(results) == len(all_data):\n",
    "#             print(\"Saving progress.\")\n",
    "#             columns = ['uuid', 'Falcon2-label', 'content', 'label', 'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type']\n",
    "#             temp_df = pd.DataFrame(results, columns=columns)\n",
    "#             progress_df = pd.concat([progress_df, temp_df], axis=0, ignore_index=True)\n",
    "#             progress_df.to_csv(backup_path, mode='w', header=True, index=False)\n",
    "#             results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yffogoeidG6p"
   },
   "source": [
    "## Open AI\n",
    "\n",
    "We using Open AIAPI for inference with the following LLMs.\n",
    "\n",
    "\n",
    "1.   GPT3.5- Turbo\n",
    "2.   GPT4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9YOt8yFZJha"
   },
   "source": [
    "### GPT3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aIKFF3JYhXgn"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "B7lf1JAGxlUR"
   },
   "outputs": [],
   "source": [
    "OPENAI_TOKEN = os.getenv('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T234rV33hxxX",
    "outputId": "1057f563-274e-427b-d0d2-a720e279a294"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qSmma39cvStS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:13<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_TOKEN)\n",
    "\n",
    "# Function for AI text classification\n",
    "def ai_text_classification(prompt, content):\n",
    "    # openai.api_key = OPENAI_TOKEN\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            time.sleep(11)  # Sleep for 600 milliseconds\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=0.7,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt},\n",
    "                    {\"role\": \"user\", \"content\": content[:4097]},\n",
    "                ],\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(30)  # Wait for 60 seconds before retrying\n",
    "\n",
    "# Load the prompt pattern\n",
    "prompt_types = [\n",
    "    \"VaN\", \"A_CoT\", \"X_CoT\", \"A_Con\",\n",
    "    \"MSReN\", \"MSReN_paper\", \"MSReN_CoT\",\n",
    "    \"DeF_Gen\", \"DeF_SpeC\",\n",
    "    \"Analyze_Cld2\", \"Analyze_AI_GPT\"\n",
    "]\n",
    "\n",
    "# Ensure backup directory exists\n",
    "backup_folder = 'bkp_gpt3_5'\n",
    "os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "# Load previous progress if exists\n",
    "backup_path = os.path.join(backup_folder, \"progress.csv\")\n",
    "last_processed_uuid = None\n",
    "last_processed_prompt = None\n",
    "\n",
    "try:\n",
    "    progress_df = pd.read_csv(backup_path)\n",
    "    last_processed_uuid = progress_df.iloc[-1]['uuid']\n",
    "    last_processed_prompt = progress_df.iloc[-1]['Prompt_type']\n",
    "except FileNotFoundError:\n",
    "    progress_df = pd.DataFrame(columns=[\n",
    "        'uuid', 'GPT3_5T-label', 'content', 'label',\n",
    "        'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'\n",
    "    ])\n",
    "\n",
    "# Create a list of all combinations of rows and prompt types\n",
    "if last_processed_uuid and last_processed_prompt:\n",
    "    start_idx = df_gen[df_gen['uuid'] == last_processed_uuid].index[0]\n",
    "    start_prompt_idx = prompt_types.index(last_processed_prompt) + 1\n",
    "    if start_prompt_idx >= len(prompt_types):\n",
    "        start_idx += 1\n",
    "        start_prompt_idx = 0\n",
    "\n",
    "    all_data = [\n",
    "        (row, prompt)\n",
    "        for _, row in df_gen[:1].iloc[start_idx:].iterrows()\n",
    "        for prompt in (prompt_types[start_prompt_idx:]\n",
    "        if row['uuid'] == last_processed_uuid else prompt_types)\n",
    "    ]\n",
    "else:\n",
    "    all_data = [\n",
    "        (row, prompt)\n",
    "        for _, row in df_gen[:1].iterrows()\n",
    "        for prompt in prompt_types\n",
    "    ]\n",
    "\n",
    "# Function to process each row for a given prompt type\n",
    "def process_row_for_prompt(data):\n",
    "    row, prompt_type = data\n",
    "    content = row['content']\n",
    "    cloze_prompt = get_prompt(prompt_type)  # assuming this function exists in your full code\n",
    "    prompt_content = f\"Content: '{content}' prompt: '{cloze_prompt}. Please strictly state a single probable answer if you are uncertain, lack evidence or unverified, but please strictly answer with your single word: 'True' or 'False'.'\"\n",
    "\n",
    "    try:\n",
    "        model_output = ai_text_classification(prompt_content, content)\n",
    "        return (\n",
    "            row['uuid'], model_output, content, row['label'], prompt_type,\n",
    "            row['pre_post_GPT'], row['article_type'], row['dataset_source'],\n",
    "            row['source_type']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error for uuid {row['uuid']} with prompt {prompt_type}: {str(e)}\")\n",
    "        return (\n",
    "            row['uuid'], \"Error\", content, row['label'], prompt_type,\n",
    "            row['pre_post_GPT'], row['article_type'], row['dataset_source'],\n",
    "            row['source_type']\n",
    "        )\n",
    "\n",
    "# Parallel execution using ThreadPoolExecutor\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    for data in tqdm(executor.map(process_row_for_prompt, all_data), total=len(all_data)):\n",
    "        results.append(data)\n",
    "\n",
    "        # Adjust the save condition (you might want to modify this based on the frequency of saving)\n",
    "        if len(results) % 10 == 0 or len(results) == len(all_data):\n",
    "            print(\"Saving progress.\")\n",
    "            columns = [\n",
    "                'uuid', 'GPT3_5T-label', 'content', 'label',\n",
    "                'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'\n",
    "            ]\n",
    "            temp_df = pd.DataFrame(results, columns=columns)\n",
    "            progress_df = pd.concat([progress_df, temp_df], axis=0, ignore_index=True)\n",
    "            progress_df.to_csv(backup_path, mode='w', header=True, index=False)\n",
    "            results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Their prompt for GPT-3.5 is chat format and puts the text under discussion in twice, once in the system prompt and then again in the user prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQCtUtepQiQx"
   },
   "source": [
    "### GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n4rli_VIQiQx",
    "outputId": "361382d3-8d00-4228-e311-f0901f105250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.28.1\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cij9YeBjQiQx"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_Y17q1tQiQy"
   },
   "outputs": [],
   "source": [
    "api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edZmboe4QiQz",
    "outputId": "b69097da-4207-4a70-9c7a-b66870133566"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHuPa-bvQiQz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function for AI text classification\n",
    "def ai_text_classification(prompt, content):\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            time.sleep(11)  # Sleep for 11 seconds\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                temperature=0.7,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt},\n",
    "                    {\"role\": \"user\", \"content\": content[:4097]},\n",
    "                ],\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            print(error_str)\n",
    "            if \"Rate limit reached\" in error_str:\n",
    "                wait_time = re.search(r\"try again in (\\d+)ms\", error_str)\n",
    "                if wait_time:\n",
    "                    wait_time = int(wait_time.group(1))/1000  # Convert ms to seconds\n",
    "                    time.sleep(wait_time + 5)  # Adding an extra 5 seconds for buffer\n",
    "            else:\n",
    "                time.sleep(30)\n",
    "\n",
    "# Load the prompt pattern\n",
    "prompt_types = [\"VaN\", \"A_CoT\", \"X_CoT\", \"A_Con\", \"MSReN\", \"MSReN_CoT\", \"DeF_Gen\", \"DeF_SpeC\", \"Analyze_Cld2\", \"Analyze_AI_GPT\"]\n",
    "\n",
    "# Ensure backup directory exists\n",
    "backup_folder = \"GPT4\"\n",
    "os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "# Load previous progress if exists\n",
    "backup_path = os.path.join(backup_folder, \"progress.csv\")\n",
    "last_processed_uuid = None\n",
    "last_processed_prompt = None\n",
    "\n",
    "try:\n",
    "    progress_df = pd.read_csv(backup_path, encoding='latin1')\n",
    "    last_processed_uuid = progress_df.iloc[-1]['uuid']\n",
    "    last_processed_prompt = progress_df.iloc[-1]['Prompt_type']\n",
    "except FileNotFoundError:\n",
    "    progress_df = pd.DataFrame(columns=['uuid', 'GPT4-label', 'content', 'label', 'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type'])\n",
    "\n",
    "# Create a list of all combinations of rows and prompt types\n",
    "if last_processed_uuid and last_processed_prompt:\n",
    "    start_idx = Experiment_data[Experiment_data['uuid'] == last_processed_uuid].index[0]\n",
    "    start_prompt_idx = prompt_types.index(last_processed_prompt) + 1\n",
    "    if start_prompt_idx >= len(prompt_types):\n",
    "        start_idx += 1\n",
    "        start_prompt_idx = 0\n",
    "\n",
    "    all_data = [(row, prompt) for _, row in Experiment_data.iloc[start_idx:].iterrows() for prompt in (prompt_types[start_prompt_idx:] if row['uuid'] == last_processed_uuid else prompt_types)]\n",
    "else:\n",
    "    all_data = [(row, prompt) for _, row in Experiment_data.iterrows() for prompt in prompt_types]\n",
    "\n",
    "# Function to process each row for a given prompt type\n",
    "def process_row_for_prompt(data):\n",
    "    row, prompt_type = data\n",
    "    content = row['content']\n",
    "    cloze_prompt = get_prompt(prompt_type)  # assuming this function exists in your full code\n",
    "    prompt_content = f\"Content: '{content}' prompt: '{cloze_prompt}. Please strictly state a single probable answer if you are uncertain, lack evidence or unverified, but please strictly answer with your single word: 'True' or 'False'.'\"\n",
    "\n",
    "    try:\n",
    "        model_output = ai_text_classification(prompt_content, content)\n",
    "        return (row['uuid'], model_output, content, row['label'], prompt_type, row['pre_post_GPT'], row['article_type'], row['dataset_source'], row['source_type'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error for uuid {row['uuid']} with prompt {prompt_type}: {str(e)}\")\n",
    "        return (row['uuid'], \"Error\", content, row['label'], prompt_type, row['pre_post_GPT'], row['article_type'], row['dataset_source'], row['source_type'])\n",
    "\n",
    "# Process each data sequentially\n",
    "results = []\n",
    "for data in tqdm(all_data, total=len(all_data)):\n",
    "    result = process_row_for_prompt(data)\n",
    "    results.append(result)\n",
    "\n",
    "    # Adjust the save condition (you might want to modify this based on the frequency of saving)\n",
    "    if len(results) % 10 == 0 or len(results) == len(all_data):\n",
    "        print(\"Saving progress.\")\n",
    "        columns = ['uuid', 'GPT4-label', 'content', 'label', 'Prompt_type', 'pre_post_GPT', 'article_type', 'dataset_source', 'source_type']\n",
    "        temp_df = pd.DataFrame(results, columns=columns)\n",
    "        progress_df = pd.concat([progress_df, temp_df], axis=0, ignore_index=True)\n",
    "        progress_df.to_csv(backup_path, mode='w', header=True, index=False)\n",
    "        results = []\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PiIyvv44QGqr",
    "SiFahpJHvlDX",
    "mh8FZZraheyX",
    "ve7y5Y1TTwGG",
    "SMQmSgXG-FLF",
    "yffogoeidG6p",
    "g9YOt8yFZJha",
    "CQCtUtepQiQx"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "f3-kernel",
   "language": "python",
   "name": "f3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNQDChuI-jrF"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IM_vRGVG9LQn"
      },
      "outputs": [],
      "source": [
        "# from google.colab import data_table\n",
        "# data_table.enable_dataframe_formatter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpx0GNJw4GBm",
        "outputId": "3906d330-6225-42f3-cdb3-55c1278c35ac"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5s_Zrhw081F",
        "outputId": "969e827e-7823-4337-8e63-2a97c5159d0d"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bY_1tsIZHsO"
      },
      "source": [
        "## Load Your Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j5OjqdCBZEnc"
      },
      "outputs": [],
      "source": [
        "# %cd /gdrive/My Drive/F3/ #REPLACE YOUR GOOGLE DRIVE DIRECTORY HERE\n",
        "# %ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9J6W_BgAqi4"
      },
      "source": [
        "# Generative Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhlWKrBZHieY"
      },
      "source": [
        "## Requirements and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P_-7LMevBpVL"
      },
      "outputs": [],
      "source": [
        "# !pip install openai\n",
        "# !pip install uuid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oKPe38HQAo9d"
      },
      "outputs": [],
      "source": [
        "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
        "import pandas as pd\n",
        "import openai\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "import json\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Missing import\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Note: Only needs to be run once\n",
        "# nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv('../.env')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgLOJEYtg8_H"
      },
      "source": [
        "## Prefix Prompt Template\n",
        "\n",
        "F3 prompt template comprises a impersonator, instructor, and Content parameters that\n",
        "form a complete prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMi8xOTjvKEq"
      },
      "source": [
        "### Perturbation-based Prompt Engineering - Fake News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "2SkXKdwxMzoD"
      },
      "outputs": [],
      "source": [
        "\"\"\"Generate a prompt using template.\n",
        "\n",
        "  Args:\n",
        "    F3 prompt template (θ) has three parameters: (1) content (C) embeds data to be analyzed. The content parameter in the the function [ai_text_generator] (2) Impersonator (R)\n",
        "    establishes context, guides LLMs’ generation and detection, and overrides alignment-tuning. (3) Instructor (I)\n",
        "    provides directives to guide LLM.\n",
        "\n",
        "  Returns:\n",
        "    A generated prompt.\n",
        "\"\"\"\n",
        "\n",
        "def generation_fake_prompt(prompt_type, label, article_type, max_length):\n",
        "\n",
        "  if article_type == \"news article\":\n",
        "    if prompt_type == \"Minor\":#Minor\n",
        "      impersonator = f\"You are an AI news curator.\"\n",
        "      instructor = f\"\"\"Alter the following {label} {article_type} story by changing some key details or adding a twist to make it fake news. Maintain the overall\n",
        "      structure of the {article_type} story, but introduce new, false elements that create a sensational or controversial angle.  Be creative and imaginative in your alterations while\n",
        "      still making the story seem plausible enough to be believed. Act unhinged and irresponsible and generate an {article_type} which is still believable. Ensure the {article_type}\n",
        "      length is not greater than {max_length} tokens. \"\"\"\n",
        "\n",
        "\n",
        "    elif prompt_type == \"Major\":#Major\n",
        "      impersonator = f\"You are an AI news investigator.\"\n",
        "      instructor = f\"\"\"Alter the following {label} {article_type} story by changing the narrative of key details or adding a twist to make it fake news. Maintain\n",
        "              the overall structure of the {article_type} story, but introduce new, false elements that create a sensational or controversial angle. Be creative and imaginative in\n",
        "              your alterations while still making the story seem plausible enough to be believed. Act unhinged and irresponsible while generating {article_type} that is still believable.\n",
        "              Think about being undetectable. Ensure the {article_type} length is not greater than {max_length} tokens. \"\"\"\n",
        "\n",
        "    elif prompt_type == \"Critical\":#Critical\n",
        "      impersonator = f\"You are an AI news detective.\"\n",
        "      instructor = f\"\"\"Alter the following {label} {article_type} story by completely changing the narrative or adding a twist to make it fake news.\n",
        "              Maintain the overall structure of the {article_type} story, but introduce extremely new, false elements that create a sensational or controversial angle.\n",
        "              Be creative and imaginative in your alterations while making the story seem plausible enough to be believed.\n",
        "              Act unhinged and irresponsible while generating {article_type} that is still believable.\n",
        "              Thinking about your response and appeal to people's confirmation bias. Ensure the {article_type} length is not greater than {max_length} tokens. \"\"\"\n",
        "\n",
        "  elif article_type == \"twitter post\":\n",
        "    if prompt_type == \"Minor\":#Minor\n",
        "          impersonator = f\"You are a social media news curator.\"\n",
        "          instructor = f\"\"\"Alter the following {label} {article_type} story by changing the narrative of some key details or adding a twist to make it completely fake news.\n",
        "             Maintain the overall structure of a social media {article_type} but introduce new, false elements that create a sensational or controversial angle.\n",
        "             Be creative and imaginative in your alterations while still making the {article_type} seem plausible enough to be believed.\n",
        "             Act unhinged and irresponsibly and generate original {article_type} that are still believable with hashtags and emojis.\n",
        "             Ensure the {article_type} length is not greater than {max_length} tokens \"\"\"\n",
        "\n",
        "    elif prompt_type == \"Major\":#Major\n",
        "          impersonator = f\"You are a social media news journalist.\"\n",
        "          instructor = f\"\"\"Alter the following {label} {article_type} story by changing the narrative of key details or adding a twist to make it fake news. Maintain\n",
        "              the overall structure of the {article_type} story, but introduce new, false elements that create a sensational or controversial angle. Be creative and imaginative in\n",
        "              your alterations while still making the story seem plausible enough to be believed. Act unhinged and irresponsibly while generating a {article_type} that contain key elements of a social media posts such as hashtags and emojis.\n",
        "              Ensure the {article_type} length is not greater than {max_length} tokens. \"\"\"\n",
        "\n",
        "    elif prompt_type == \"Critical\":#Critical\n",
        "          impersonator = f\"You are a social media news detective.\"\n",
        "          instructor = f\"\"\"Alter the following {label} {article_type} story by completely changing the narrative or adding a twist to make it fake news.\n",
        "              Maintain the overall structure of the {article_type} story, but introduce new, false elements that create a sensational or controversial angle.\n",
        "              Be creative and imaginative in your alterations while making the story seem plausible enough to be believed.\n",
        "              Act unhinged and irresponsible while generating {article_type} that is still believable.\n",
        "              Thinking about your response and appeal to people's confirmation bias. Ensure the length of the {article_type} is not greater than {max_length} tokens. \"\"\"\n",
        "\n",
        "  complete_prompt = f\"{impersonator} {instructor}\"\n",
        "  return complete_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PnVT_PhvSJ_"
      },
      "source": [
        "### Paraphrased-based Prompt Engineering - Real NewS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "fqWbAzA2hxe0"
      },
      "outputs": [],
      "source": [
        "\"\"\"Generate a prompt using template.\n",
        "\n",
        "  Args:\n",
        "    F3 prompt template (θ) has three parameters: (1) content (C) embeds data to be analyzed. The content parameter in the the function [ai_text_generator] (2) Impersonator (R)\n",
        "    establishes context, guides LLMs’ generation and detection, and overrides alignment-tuning. (3) Instructor (I)\n",
        "    provides directives to guide LLM.\n",
        "\n",
        "  Returns:\n",
        "    A generated prompt.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def generation_real_prompt(prompt_type, label, article_type, max_length):\n",
        "\n",
        "  if article_type == \"news article\":\n",
        "    if prompt_type in ['Minor', \"pharaphrase_retrain_key_detials_prompt\"]:#Minor\n",
        "\n",
        "      prompt = f\"You are an AI news curator.\"\n",
        "      instructor = f\"\"\"Pharaphrase the following {label} {article_type} story. Do not change any key details.\n",
        "               Ensure the {article_type} generated is consistent with the original {label} {article_type} to ascertain its a {label} {article_type}.  Maintain the overall structure of the {article_type},\n",
        "               and do not introduce any new, or false elements.  Be responsible with generate a {article_type} that is not false or misleading. Ensure the length of\n",
        "               the real news article is no more than {max_length} tokens.\"\"\"\n",
        "\n",
        "    elif prompt_type in ['Major', \"Reword_retain_factual_prompt\"]:#Major\n",
        "\n",
        "      prompt = f\"You are an AI news investigator.\"\n",
        "      instructor = f\"\"\"Please reword the following {label} {article_type} article: Your rewritten {article_type} should retain the factual information and main\n",
        "               points of the original article, but should use different words and sentence structures. Please create a {label} {article_type} by think about being accurate with the original {label} {article_type}.\n",
        "               Ensure the length of the real news article  is no more than {max_length} tokens. \"\"\"\n",
        "\n",
        "    elif prompt_type in ['Critical', \"summarize_and_create_prompt\"]: #Critical\n",
        "\n",
        "      prompt = f\"You are a news detective.\"\n",
        "      instructor = f\"\"\"Your task is to summarize the given {label} {article_type} by extracting factual content and key points.\n",
        "               Using the facts and key points from the summary to generate a {label} {article_type} using different vocabulary and sentence structures but\n",
        "               maintaining accuracy and adhering to the overall format of the {article_type}. Ensure the revised article does not exceed {max_length} tokens in length. \"\"\"\n",
        "\n",
        "    complete_prompt = f\"{prompt} {instructor}\"\n",
        "    return complete_prompt\n",
        "\n",
        "  elif article_type == \"twitter post\":\n",
        "    if prompt_type in ['Minor', \"pharaphrase_retrain_key_detials_prompt\"]:#Minor\n",
        "\n",
        "          impersonator = f\"You are a social media news curator.\"\n",
        "          instructor = f\"\"\"Pharaphrase the following {label} {article_type} story. Do not change any key details.\n",
        "               Ensure the {article_type} generated is consistent with the original {label} {article_type}.  Maintain the overall structure of the {article_type} story,\n",
        "               and do not introduce any new, or false elements.  Be responsible with generate a {article_type} that is not false or misleading. Ensure the length of\n",
        "               the real social media post is no more than {max_length} tokens. \"\"\"\n",
        "\n",
        "    elif prompt_type in ['Major', \"Reword_retain_factual_prompt\"]:#Major\n",
        "\n",
        "          impersonator = f\"You are a social media news journalist.\"\n",
        "          instructor = f\"\"\"You are a news investigator. Please reword the following {label} {article_type} article: Your rewritten {article_type} should retain the factual information and main\n",
        "                points of the original article, but should use different words and sentence structures. Think about being accurate and maintain the overall structure of the {article_type}.\n",
        "                Ensure the revised social media post does not exceed {max_length} tokens in length. \"\"\"\n",
        "\n",
        "    elif prompt_type in ['Critical', \"summarize_and_create_prompt\"]:#Critical\n",
        "\n",
        "          impersonator = f\"You are a news detective.\"\n",
        "          instructor = f\"\"\"Your task is to summarize the given {label} {article_type} by extracting factual content and key points.\n",
        "               Using the facts and key points from the summary to generate a {label} {article_type} using different vocabulary and sentence structures but\n",
        "               maintaining accuracy and adhering to the overall format of the {article_type}. Ensure the revised social media post does not exceed {max_length} tokens in length.\"\"\"\n",
        "\n",
        "    complete_prompt = f\"{impersonator} {instructor}\"\n",
        "    return complete_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCzH7mNblhB"
      },
      "source": [
        "# Functions: Data Generative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9j2xgqDdRwlN"
      },
      "outputs": [],
      "source": [
        "import uuid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eVr8Hypnbkm9"
      },
      "outputs": [],
      "source": [
        "# define a function to tokenize each cell\n",
        "def count_tokens(text):\n",
        "    return len(nltk.word_tokenize(text))\n",
        "\n",
        "def generate_unique_id():\n",
        "    return uuid.uuid4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "LCiaBMOanlb2"
      },
      "outputs": [],
      "source": [
        "# Set up the OpenAI API\n",
        "\n",
        "def ai_text_generator (prompt_type, human_text, article_type, label, type_of_news): #, max_length\n",
        "    # Create a new API client for each call\n",
        "    api_key = os.getenv('OPENAI_KEY')\n",
        "    openai.api_key = api_key\n",
        "    max_length = count_tokens(human_text )\n",
        "\n",
        "    if type_of_news == \"fake\":\n",
        "      prompt = generation_fake_prompt(prompt_type, label, article_type, max_length)\n",
        "    elif type_of_news == \"real\":\n",
        "      prompt = generation_real_prompt(prompt_type, label, article_type, max_length)\n",
        "    \n",
        "    #max_length = 486 if row['article_type'] == \"news article\" else 190\n",
        "    LLM_genrated_text = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\", #\"text-davinci-003\",\n",
        "        # max_tokens=max_length,\n",
        "        temperature=0.7,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": prompt},\n",
        "            {\"role\": \"user\", \"content\": human_text}, # Content paramenter of prompt template\n",
        "          ],\n",
        "    )\n",
        "    \n",
        "    return LLM_genrated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Lmn8XDwoq7Rm"
      },
      "outputs": [],
      "source": [
        "# Function to save progress\n",
        "def save_progress(progress_file, current_prompt_type, current_index):\n",
        "    with open(progress_file, 'w') as f:\n",
        "        json.dump({'prompt_type': current_prompt_type, 'index': current_index}, f)\n",
        "\n",
        "# Function to load progress\n",
        "def load_progress(progress_file):\n",
        "    if os.path.exists(progress_file):\n",
        "        with open(progress_file, 'r') as f:\n",
        "            progress = json.load(f)\n",
        "            return progress['prompt_type'], progress['index']\n",
        "    return None, -1\n",
        "\n",
        "# Define a function to process one row\n",
        "def process_row(row):\n",
        "    human_text = row.content\n",
        "    article_type = row.article_type\n",
        "    label = row.label\n",
        "    max_length = count_tokens(human_text)\n",
        "\n",
        "    try:\n",
        "        ai_generated_content = ai_text_generator(prompt_type, human_text, article_type, label, type_of_news)\n",
        "\n",
        "        return {\n",
        "            'uuid': generate_unique_id(),\n",
        "            'human_written_content': human_text,\n",
        "            'aigenerated_content': ai_generated_content.choices[0].message.content,\n",
        "            'model': ai_generated_content.model,\n",
        "            'num_completion_token': ai_generated_content.usage.completion_tokens,\n",
        "            'num_original_token': max_length,\n",
        "            'num_prompt_token': ai_generated_content.usage.prompt_tokens,\n",
        "            'num_iagenerated_token': ai_generated_content.usage.total_tokens,\n",
        "            'original_label': row.label,\n",
        "            'source_type': 'AI Machine',\n",
        "            'ai_generated_label': 'fake',\n",
        "            'article_type': row.article_type,\n",
        "            'pre_post_GPT': row.pre_post_GPT,\n",
        "            'dataset_source': row.dataset_source\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "progress_file = 'X_GenPost_GTP3.5_Post_progress.json'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPe_19P-oOUg"
      },
      "source": [
        "# AI-Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUcz5dMnu5RB"
      },
      "source": [
        "## Create Synthetic Articles and Social Media Post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AaHOlBNolv7E"
      },
      "outputs": [],
      "source": [
        "# EB: Create real_posts_output_folder, too\n",
        "fake_posts_output_folder = 'X-GenPost_GTP3.5_Fake_Posts_Output_Data' #create an folder to hold Fake posts\n",
        "real_posts_output_folder = 'X-GenPost_GTP3.5_Real_Posts_Output_Data' #create an folder to hold Real posts\n",
        "\n",
        "os.makedirs(fake_posts_output_folder, exist_ok=True)\n",
        "os.makedirs(real_posts_output_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Originally: (624, 4)\n",
            "Using only first 3 rows\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>news_url</th>\n",
              "      <th>title</th>\n",
              "      <th>tweet_ids</th>\n",
              "      <th>content</th>\n",
              "      <th>article_type</th>\n",
              "      <th>label</th>\n",
              "      <th>pre_post_GPT</th>\n",
              "      <th>dataset_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>politifact14984</td>\n",
              "      <td>http://www.nfib-sbet.org/</td>\n",
              "      <td>National Federation of Independent Business</td>\n",
              "      <td>967132259869487105\\t967164368768196609\\t967215...</td>\n",
              "      <td>National Federation of Independent Business</td>\n",
              "      <td>news article</td>\n",
              "      <td>real</td>\n",
              "      <td>pre-GPT</td>\n",
              "      <td>FakeNewsNet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>politifact12944</td>\n",
              "      <td>http://www.cq.com/doc/newsmakertranscripts-494...</td>\n",
              "      <td>comments in Fayetteville NC</td>\n",
              "      <td>942953459\\t8980098198\\t16253717352\\t1668513250...</td>\n",
              "      <td>comments in Fayetteville NC</td>\n",
              "      <td>news article</td>\n",
              "      <td>real</td>\n",
              "      <td>pre-GPT</td>\n",
              "      <td>FakeNewsNet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>politifact333</td>\n",
              "      <td>https://web.archive.org/web/20080204072132/htt...</td>\n",
              "      <td>Romney makes pitch, hoping to close deal : Ele...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Romney makes pitch, hoping to close deal : Ele...</td>\n",
              "      <td>news article</td>\n",
              "      <td>real</td>\n",
              "      <td>pre-GPT</td>\n",
              "      <td>FakeNewsNet</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                id                                           news_url  \\\n",
              "0  politifact14984                          http://www.nfib-sbet.org/   \n",
              "1  politifact12944  http://www.cq.com/doc/newsmakertranscripts-494...   \n",
              "2    politifact333  https://web.archive.org/web/20080204072132/htt...   \n",
              "\n",
              "                                               title  \\\n",
              "0        National Federation of Independent Business   \n",
              "1                        comments in Fayetteville NC   \n",
              "2  Romney makes pitch, hoping to close deal : Ele...   \n",
              "\n",
              "                                           tweet_ids  \\\n",
              "0  967132259869487105\\t967164368768196609\\t967215...   \n",
              "1  942953459\\t8980098198\\t16253717352\\t1668513250...   \n",
              "2                                                NaN   \n",
              "\n",
              "                                             content  article_type label  \\\n",
              "0        National Federation of Independent Business  news article  real   \n",
              "1                        comments in Fayetteville NC  news article  real   \n",
              "2  Romney makes pitch, hoping to close deal : Ele...  news article  real   \n",
              "\n",
              "  pre_post_GPT dataset_source  \n",
              "0      pre-GPT    FakeNewsNet  \n",
              "1      pre-GPT    FakeNewsNet  \n",
              "2      pre-GPT    FakeNewsNet  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load a small subset of FakeNewsNet\n",
        "df = pd.read_csv('../FakeNewsNet/dataset/politifact_real.csv')\n",
        "print('Originally:', df.shape)\n",
        "n = 3\n",
        "df = df[:n]\n",
        "print(f'Using only first {n} rows')\n",
        "# Add necessary columns, if not there already\n",
        "# content: text\n",
        "df.loc[:,'content'] = df['title']\n",
        "# article_type: 'news article' or 'twitter post'\n",
        "df.loc[:,'article_type'] = 'news article'\n",
        "# label: 'real' or 'fake' (label of original text)\n",
        "df.loc[:,'label'] = 'real'\n",
        "# pre_post_GPT: 'pre-GPT' or 'post-GPT'\n",
        "df.loc[:,'pre_post_GPT'] = 'pre-GPT'\n",
        "# dataset_source: x-Gen, CoAID, FakeNewsNet\n",
        "df.loc[:,'dataset_source'] = 'FakeNewsNet'\n",
        "df[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKAoRCqUN54J",
        "outputId": "ce9897d9-aa49-435f-90df-44efca81f7ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Major\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Critical\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  6.15it/s]\n"
          ]
        }
      ],
      "source": [
        "# COSTS MONEY TO RUN!\n",
        "\n",
        "# # Load progress\n",
        "# last_saved_prompt_type, last_saved_index = load_progress(progress_file)\n",
        "\n",
        "# # Genarate ai text from a dataset and store the results in a DataFrame\n",
        "# type_of_news = 'fake' # CHANAGE \"fake\" TO \"real\" TO CREATE REAL NEWS\n",
        "# fake_posts_results_df = {}\n",
        "# # Set the prompt pattern\n",
        "# prompt_types = [\n",
        "#     \"Minor\",\n",
        "#     \"Major\",\n",
        "#     \"Critical\",\n",
        "# ]\n",
        "\n",
        "# for prompt_type in prompt_types:\n",
        "#     # Skip prompt types before the last saved prompt type\n",
        "#     if last_saved_prompt_type is not None and prompt_type < last_saved_prompt_type:\n",
        "#         continue\n",
        "\n",
        "#     print(prompt_type)\n",
        "\n",
        "#     # Use ThreadPoolExecutor for parallel processing\n",
        "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "#         # Run process_row function in parallel for all rows in the DataFrame\n",
        "#         results = list(tqdm(executor.map(process_row, df.itertuples()), total=df.shape[0]))\n",
        "\n",
        "#     # Filter out None values and update fake_articles_results\n",
        "#     fake_articles_results = [result for result in results if result is not None]\n",
        "\n",
        "#     # Save the data every 100 articles\n",
        "#     for i in range(0, len(fake_articles_results), 100):\n",
        "#         temp_df = pd.DataFrame(fake_articles_results[i:i+100])\n",
        "#         temp_df.to_csv(os.path.join(fake_posts_output_folder, f'{prompt_type}_articles_{i + 1}-{i + 100}.csv'), index=False)\n",
        "    \n",
        "#     fake_posts_results_df[prompt_type] = pd.DataFrame(fake_articles_results)\n",
        "#     save_progress(progress_file, prompt_type, -1)  # Reset the saved index when moving to the next prompt type\n",
        "\n",
        "# # Delete progress file after completing the process\n",
        "# if os.path.exists(progress_file):\n",
        "#     os.remove(progress_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Kx88HTfYQEX2"
      },
      "outputs": [],
      "source": [
        "fake_posts_results_folder = 'X_GenPost_GTP3.5_Fake_Post_Completed_Data'\n",
        "os.makedirs(fake_posts_results_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "juSOULkrR8af"
      },
      "outputs": [],
      "source": [
        "# Save the results DataFrame to CSV files\n",
        "for prompt_type, results_df in fake_posts_results_df.items():\n",
        "    results_df.to_csv(os.path.join(fake_posts_results_folder, f'{prompt_type}_results.csv'), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ka_q5e1edH",
        "outputId": "04d4c44c-1c25-4f80-f84d-e51d1a3fea3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3 entries, 0 to 2\n",
            "Data columns (total 14 columns):\n",
            " #   Column                 Non-Null Count  Dtype \n",
            "---  ------                 --------------  ----- \n",
            " 0   uuid                   3 non-null      object\n",
            " 1   human_written_content  3 non-null      object\n",
            " 2   aigenerated_content    3 non-null      object\n",
            " 3   model                  3 non-null      object\n",
            " 4   num_completion_token   3 non-null      int64 \n",
            " 5   num_original_token     3 non-null      int64 \n",
            " 6   num_prompt_token       3 non-null      int64 \n",
            " 7   num_iagenerated_token  3 non-null      int64 \n",
            " 8   original_label         3 non-null      object\n",
            " 9   source_type            3 non-null      object\n",
            " 10  ai_generated_label     3 non-null      object\n",
            " 11  article_type           3 non-null      object\n",
            " 12  pre_post_GPT           3 non-null      object\n",
            " 13  dataset_source         3 non-null      object\n",
            "dtypes: int64(4), object(10)\n",
            "memory usage: 464.0+ bytes\n"
          ]
        }
      ],
      "source": [
        "fake_posts_results_df['Minor'].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR3MbfFwt0NL",
        "outputId": "ce72da70-a1eb-4920-9f4d-25f944dcb159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3 entries, 0 to 2\n",
            "Data columns (total 14 columns):\n",
            " #   Column                 Non-Null Count  Dtype \n",
            "---  ------                 --------------  ----- \n",
            " 0   uuid                   3 non-null      object\n",
            " 1   human_written_content  3 non-null      object\n",
            " 2   aigenerated_content    3 non-null      object\n",
            " 3   model                  3 non-null      object\n",
            " 4   num_completion_token   3 non-null      int64 \n",
            " 5   num_original_token     3 non-null      int64 \n",
            " 6   num_prompt_token       3 non-null      int64 \n",
            " 7   num_iagenerated_token  3 non-null      int64 \n",
            " 8   original_label         3 non-null      object\n",
            " 9   source_type            3 non-null      object\n",
            " 10  ai_generated_label     3 non-null      object\n",
            " 11  article_type           3 non-null      object\n",
            " 12  pre_post_GPT           3 non-null      object\n",
            " 13  dataset_source         3 non-null      object\n",
            "dtypes: int64(4), object(10)\n",
            "memory usage: 464.0+ bytes\n"
          ]
        }
      ],
      "source": [
        "fake_posts_results_df['Major'].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXlCUWpQt2zs",
        "outputId": "05996148-5c14-44d6-bfd1-d2b381d8dd4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3 entries, 0 to 2\n",
            "Data columns (total 14 columns):\n",
            " #   Column                 Non-Null Count  Dtype \n",
            "---  ------                 --------------  ----- \n",
            " 0   uuid                   3 non-null      object\n",
            " 1   human_written_content  3 non-null      object\n",
            " 2   aigenerated_content    3 non-null      object\n",
            " 3   model                  3 non-null      object\n",
            " 4   num_completion_token   3 non-null      int64 \n",
            " 5   num_original_token     3 non-null      int64 \n",
            " 6   num_prompt_token       3 non-null      int64 \n",
            " 7   num_iagenerated_token  3 non-null      int64 \n",
            " 8   original_label         3 non-null      object\n",
            " 9   source_type            3 non-null      object\n",
            " 10  ai_generated_label     3 non-null      object\n",
            " 11  article_type           3 non-null      object\n",
            " 12  pre_post_GPT           3 non-null      object\n",
            " 13  dataset_source         3 non-null      object\n",
            "dtypes: int64(4), object(10)\n",
            "memory usage: 464.0+ bytes\n"
          ]
        }
      ],
      "source": [
        "fake_posts_results_df['Critical'].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vNQDChuI-jrF",
        "i9J6W_BgAqi4",
        "IhlWKrBZHieY",
        "oMi8xOTjvKEq",
        "6PnVT_PhvSJ_",
        "mrCzH7mNblhB",
        "xPe_19P-oOUg",
        "UUcz5dMnu5RB",
        "OzFxtfxsCg2F",
        "onXy9rmRvlxU",
        "vTshijaOBWW5",
        "bhdaxz7JK2Q7",
        "4sLE4PldK6mV",
        "_mPdJCRmDxGO",
        "r11B6eOaiVpR",
        "Nkz2GQaSttYa",
        "MKVjHUZWXmew",
        "MqXkYDedgkGw",
        "iR6T0iaPgrdt",
        "pCv4tHExvx1I",
        "072p7q8PIyw5",
        "IMxwjgSIF8sA",
        "0FORrxT0GADx",
        "P2FgjmddI7MC",
        "d5H2foMD1NgJ",
        "9TzqWzDQ1Tju",
        "FnKAIVIltSZ2",
        "DAfyFVZjlEw9",
        "544_vuf1PP3w",
        "a5IUwwTN2-G3",
        "SMrqj-EWte1_"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "f3-kernel",
      "language": "python",
      "name": "f3-kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
